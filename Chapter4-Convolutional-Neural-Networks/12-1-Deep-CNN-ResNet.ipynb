{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Neural Network\n",
    "\n",
    "当大家还在惊叹GoogLeNet用结构化的连接纳入了大量卷积层的时候，微软亚洲研究院的研究员已经在设计更深但结构更简单的网络ResNet。他们凭借这个网络在2015年的Imagenet竞赛中大获全胜。\n",
    "\n",
    "ResNet有效解决了深度卷积神经网络难训练的问题。这是因为在误差反传的过程中，梯度通常变的越来越小，从而权重的更新量也变小。这个导致原理损失函数的层训练缓慢，随着层数的增加这个现象更加明显，之前有两种常用方案来尝试解决这个问题：\n",
    "\n",
    "* 按层训练。先训练靠近数据的层，然后慢慢的增加后面的层。但效果不是特别好，而且比较麻烦。\n",
    "\n",
    "* 使用更宽的层（增加输出通道）而不是更深来增加模型复杂度。但更宽的模型经常不如更深的效果好。\n",
    "\n",
    "ResNet通过增加跨层的连接来解决梯度逐层回传时变小的问题。虽然这个想法之前就提出过了，但ResNet真正的把效果做好了。\n",
    "\n",
    "下面演示了一个跨层的连接：\n",
    "\n",
    "<img src=\"http://zh.gluon.ai/_images/residual.svg\">\n",
    "\n",
    "最底下那层的输入不仅仅是输出给了中间层，而且其与中间层结果相加进入最上层。这样在梯度反传时，最上层梯度可以直接跳过中间层传到最下层，从而避免最下层梯度过小情况。\n",
    "\n",
    "为什么叫做残差网络呢？我们可以将上面示意图里的结构拆成两个网络的和，一个一层，一个两层，最下面层是共享的。\n",
    "\n",
    "<img src=\"http://zh.gluon.ai/_images/residual2.svg\">\n",
    "\n",
    "在训练过程中，左边的网络因为更简单所以更容易训练。这个小网络没有拟合到的部分，或者说残差，则被右边的网络抓取住。所以直观上来说，即使加深网络，跨层连接仍然可以使得底层网络可以充分的训练，从而不会让训练更难。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Chapter4-Convolutional-Neural-Networks/4-17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到ResNet使用了很多残差块作为其网络的基本结构，并且可以观察到，有的相邻的残差块之间需要通道数量的增加，即增加为原来的2倍，并且随之输出变为原来的1/2，这也是为了考虑每一层中计算的时间复杂度，因此，在设计残差块的时候，我们要考虑到这种通道数和数据宽度的变化。下面我们来设计一个这样的残差块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T10:42:31.660623Z",
     "start_time": "2018-01-25T10:42:31.139410Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T10:42:31.679553Z",
     "start_time": "2018-01-25T10:42:31.661858Z"
    }
   },
   "outputs": [],
   "source": [
    "class Residual(gluon.Block):\n",
    "    def __init__(self, channels, shape_inc=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.shape_inc = shape_inc\n",
    "        # 如果channel数增加，我们就要将feature map宽度减半来保持时间复杂度\n",
    "        strides = 2 if shape_inc else 1\n",
    "        self.conv1 = gluon.nn.Conv2D(channels, kernel_size=3, strides=strides, padding=1)\n",
    "        self.bn1 = gluon.nn.BatchNorm(axis=1)\n",
    "        self.conv2 = gluon.nn.Conv2D(channels, kernel_size=3, strides=1, padding=1)\n",
    "        self.bn2 = gluon.nn.BatchNorm(axis=1)\n",
    "        if shape_inc: # 如果channel数增加，则要将feature map宽度减半\n",
    "            self.conv3 = gluon.nn.Conv2D(channels, kernel_size=1, strides=strides)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        out = nd.relu(self.bn1(self.conv1(X)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.shape_inc:\n",
    "            X = self.conv3(X)\n",
    "        return nd.relu(out + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T10:42:32.800095Z",
     "start_time": "2018-01-25T10:42:31.680759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256, 14, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "residual = Residual(256, shape_inc=True)\n",
    "residual.initialize(ctx=ctx)\n",
    "X = nd.random.uniform(shape=(32, 128, 28, 28), ctx=ctx)\n",
    "residual(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好了残差块以后，下面我们实现ResNet18。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T10:42:32.840076Z",
     "start_time": "2018-01-25T10:42:32.801280Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet18(gluon.Block):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.verbose = verbose \n",
    "        with self.name_scope():\n",
    "            b1 = gluon.nn.Sequential()\n",
    "            b1.add(\n",
    "                gluon.nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "                gluon.nn.MaxPool2D(pool_size=3, strides=2, padding=1)\n",
    "            )\n",
    "            b2 = gluon.nn.Sequential()\n",
    "            b2.add(\n",
    "                Residual(64),\n",
    "                Residual(64)\n",
    "            )\n",
    "            b3 = gluon.nn.Sequential()\n",
    "            b3.add(\n",
    "                Residual(128, shape_inc=True),\n",
    "                Residual(128)\n",
    "            )\n",
    "            b4 = gluon.nn.Sequential()\n",
    "            b4.add(\n",
    "                Residual(256, shape_inc=True),\n",
    "                Residual(256)\n",
    "            )\n",
    "            b5 = gluon.nn.Sequential()\n",
    "            b5.add(\n",
    "                Residual(512, shape_inc=True),\n",
    "                Residual(512)\n",
    "            )\n",
    "            b6 = gluon.nn.Sequential()\n",
    "            b6.add(\n",
    "                gluon.nn.AvgPool2D(pool_size=7),\n",
    "                gluon.nn.Flatten(),\n",
    "                gluon.nn.Dense(num_classes)\n",
    "            )\n",
    "            \n",
    "        self.net = gluon.nn.Sequential()\n",
    "        self.net.add(b1, b2, b3, b4, b5, b6)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, blk in enumerate(self.net):\n",
    "            out = blk(out)\n",
    "            if self.verbose:\n",
    "                print(\"blk %d : %s\" % ((i+1), out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T10:42:33.140038Z",
     "start_time": "2018-01-25T10:42:32.841246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blk 1 : (32, 64, 56, 56)\n",
      "blk 2 : (32, 64, 56, 56)\n",
      "blk 3 : (32, 128, 28, 28)\n",
      "blk 4 : (32, 256, 14, 14)\n",
      "blk 5 : (32, 512, 7, 7)\n",
      "blk 6 : (32, 10)\n"
     ]
    }
   ],
   "source": [
    "resnet18 = ResNet18(10, verbose=True)\n",
    "resnet18.initialize(ctx=ctx)\n",
    "img = nd.random.normal(shape=(32, 3, 224, 224), ctx=ctx)\n",
    "y = resnet18(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T12:46:19.643225Z",
     "start_time": "2018-01-25T10:42:33.145550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Moving Train Avg loss 0.99859, Train acc 0.68838, Test acc 0.66480, Time consume 371.91644 s.\n",
      "Epoch 1, Moving Train Avg loss 0.57993, Train acc 0.82110, Test acc 0.78250, Time consume 371.53594 s.\n",
      "Epoch 2, Moving Train Avg loss 0.46610, Train acc 0.86808, Test acc 0.80640, Time consume 370.99050 s.\n",
      "Epoch 3, Moving Train Avg loss 0.36022, Train acc 0.90014, Test acc 0.80880, Time consume 371.29495 s.\n",
      "Epoch 4, Moving Train Avg loss 0.34219, Train acc 0.93746, Test acc 0.82690, Time consume 371.36973 s.\n",
      "Epoch 5, Moving Train Avg loss 0.23900, Train acc 0.93820, Test acc 0.81380, Time consume 371.33454 s.\n",
      "Epoch 6, Moving Train Avg loss 0.14782, Train acc 0.96718, Test acc 0.82470, Time consume 371.36895 s.\n",
      "Epoch 7, Moving Train Avg loss 0.11006, Train acc 0.97052, Test acc 0.82690, Time consume 371.24905 s.\n",
      "Epoch 8, Moving Train Avg loss 0.08237, Train acc 0.95556, Test acc 0.79880, Time consume 371.20606 s.\n",
      "Epoch 9, Moving Train Avg loss 0.05432, Train acc 0.99018, Test acc 0.83760, Time consume 371.23385 s.\n",
      "Epoch 10, Moving Train Avg loss 0.04662, Train acc 0.98996, Test acc 0.83740, Time consume 371.23989 s.\n",
      "Epoch 11, Moving Train Avg loss 0.01807, Train acc 0.99704, Test acc 0.84680, Time consume 371.36376 s.\n",
      "Epoch 12, Moving Train Avg loss 0.00922, Train acc 0.99940, Test acc 0.85820, Time consume 371.06264 s.\n",
      "Epoch 13, Moving Train Avg loss 0.01098, Train acc 0.99860, Test acc 0.85400, Time consume 371.17085 s.\n",
      "Epoch 14, Moving Train Avg loss 0.00942, Train acc 0.99806, Test acc 0.85010, Time consume 371.29236 s.\n",
      "Epoch 15, Moving Train Avg loss 0.00943, Train acc 0.99956, Test acc 0.85850, Time consume 371.27967 s.\n",
      "Epoch 16, Moving Train Avg loss 0.00101, Train acc 1.00000, Test acc 0.86610, Time consume 371.27105 s.\n",
      "Epoch 17, Moving Train Avg loss 0.00097, Train acc 1.00000, Test acc 0.86940, Time consume 371.18869 s.\n",
      "Epoch 18, Moving Train Avg loss 0.00095, Train acc 0.99998, Test acc 0.86820, Time consume 371.17489 s.\n",
      "Epoch 19, Moving Train Avg loss 0.00017, Train acc 1.00000, Test acc 0.87160, Time consume 371.11629 s.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "batch_size = 32\n",
    "train_data, test_data = utils.load_dataset(batch_size, resize=299, data_type='cifar10')\n",
    "\n",
    "resnet18 = ResNet18(10, verbose=False)\n",
    "resnet18.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "trainer = gluon.Trainer(resnet18.collect_params(), 'sgd', {'learning_rate' : learning_rate})\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "niter = 0\n",
    "moving_loss = 0.0\n",
    "smoothing_constant = 0.9\n",
    "\n",
    "from time import time\n",
    "for epoch in range(epochs):\n",
    "    start = time()\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = resnet18(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        niter += 1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = smoothing_constant * moving_loss + (1-smoothing_constant) * curr_loss\n",
    "        estimated_loss = moving_loss / (1 - smoothing_constant**niter)\n",
    "    \n",
    "    if not (epoch+1)%5:\n",
    "        learning_rate /= 10\n",
    "    \n",
    "    train_acc = utils.evaluate_accuracy_gluon(train_data, resnet18, ctx)\n",
    "    test_acc = utils.evaluate_accuracy_gluon(test_data, resnet18, ctx)\n",
    "    print(\"Epoch %d, Moving Train Avg loss %.5f, Train acc %.5f, Test acc %.5f, Time consume %.5f s.\"\n",
    "         % (epoch, estimated_loss, train_acc, test_acc, time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet的效果确实好啊！！！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
