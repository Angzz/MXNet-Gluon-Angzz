{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零实现多层感知机分类\n",
    "\n",
    "多层感知机和多类逻辑分类很类似，就是在之间加入隐层以提升模型精度\n",
    "\n",
    "<img src=\"http://zh.gluon.ai/_images/multilayer-perceptron.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.079990Z",
     "start_time": "2018-01-15T04:20:01.558603Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.712271Z",
     "start_time": "2018-01-15T04:20:02.084509Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    return data.astype(np.float32) / 255, label.astype(np.float32)\n",
    "    \n",
    "fashion_mnist_train = gluon.data.vision.FashionMNIST(train=True, transform=transform)\n",
    "fashion_mnist_test = gluon.data.vision.FashionMNIST(train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.723832Z",
     "start_time": "2018-01-15T04:20:02.718657Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = gluon.data.DataLoader(fashion_mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.761871Z",
     "start_time": "2018-01-15T04:20:02.729900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 28, 28, 1)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "for (data, label) in test_data:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "\n",
    "### 小随机数初始化(来自cs231n笔记)\n",
    "\n",
    "### <font color=\"red\">目前最好的权重初始化方法就是Kaiming He提出的Xaiver初始化方法<br>\n",
    "\n",
    "<font color=\"red\">**错误**</font>：全零初始化。让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数(**因为用零均值法中心化后数据的均值为0**)。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。\n",
    "\n",
    "<font color=\"red\">**小随机数初始化**</font>。因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：$W = 0.01 * np.random.randn(D,H)$。其中randn函数是基于零均值和标准差的一个高斯分布（译者注：国内教程一般习惯称均值参数为期望$\\mu$）来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。\n",
    "\n",
    "<font color=\"red\">**警告**</font>。并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。\n",
    "\n",
    "<font color=\"red\">**使用1/sqrt(n)校准方差**</font>。上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大,记输出结果为$S$，那么具体推导如下：\n",
    "\n",
    "\\begin{split}\n",
    "  & Var(s) = Var(\\sum_i^n w_ix_i) \\\\ \n",
    "= & \\sum_i^n Var(w_ix_i) \\\\ \n",
    "= & \\sum_i^n[E(w_i)]^2 Var(x_i) + [E(x_i)]^2 Var(w_i) + Var(x_i)Var(w_i) \\\\ \n",
    "= & \\sum_i^n Var(x_i)Var(w_i) \\\\ \n",
    "= & (nVar(w))Var(w) \\end{split}\n",
    "\n",
    "因此，如果想要$s$和$x$有一样的方差，则应该有$nVar(w) = 1$，则有$nVar(w) = nVar(aw^\\prime) = n \\times a^2Var(w^\\prime) = 1$，则有$a = \\sqrt{1/n}$，其中其中$w^\\prime$为方差规范化后的参数，这便是著名的**Xaiver**初始化。\n",
    "\n",
    "于是得出$w = np.random.randn(n) / sqrt(n)$.这样可以保持输出和输入的权重方差保持不变。\n",
    "\n",
    "### <font color=\"red\">改进\n",
    "\n",
    "* Glorot等在论文[Understanding the difficulty of training deep feedforward neural networks](http://202.119.95.70/cache/4/03/proceedings.mlr.press/c896b216aca8427f10edb48249b207d1/glorot10a.pdf)中作出了类似的分析。在论文中，作者推荐初始化公式为 $\\text{Var}(w) = 2/(n_{in} + n_{out}) $，其中\\(n_{in}, n_{out}\\)是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。<br>\n",
    "   \n",
    "* Kaiming He等人在[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)一文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是$2.0/n$。代码为$w = np.random.randn(n) * sqrt(n/2.0)$。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。\n",
    "\n",
    "<font color=\"red\">**稀疏初始化**</font>（Sparse initialization）。另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。\n",
    "\n",
    "<font color=\"red\">**偏置（biases）的初始化**</font>。通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。\n",
    "\n",
    "### <font color=\"red\">实践\n",
    "\n",
    "当前的推荐是使用ReLU激活函数，并且使用$w = np.random.randn(n) * sqrt(n/2.0)$来进行权重初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.798100Z",
     "start_time": "2018-01-15T04:20:02.766798Z"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 60000\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_hidden = 256\n",
    "\n",
    "weight_scale = .01\n",
    "\n",
    "# 小随机数初始化\n",
    "# W1 = nd.random.normal(shape=(num_inputs, num_hidden), scale=weight_scale)\n",
    "# b1 = nd.random.normal(shape=num_hidden)\n",
    "\n",
    "# W2 = nd.random.normal(shape=(num_hidden, num_outputs), scale=weight_scale)\n",
    "# b2 = nd.random.normal(shape=num_outputs)\n",
    "\n",
    "# He Xaiver初始化\n",
    "W1 = nd.random.normal(shape=(num_inputs, num_hidden)) / np.sqrt(num_inputs / 2)\n",
    "b1 = nd.random.normal(shape=num_hidden)\n",
    "\n",
    "W2 = nd.random.normal(shape=(num_hidden, num_outputs)) / np.sqrt(num_hidden / 2)\n",
    "b2 = nd.random.normal(shape=num_outputs)\n",
    "\n",
    "vs = []\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    vs.append(param.zeros_like())\n",
    "    param.attach_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.808628Z",
     "start_time": "2018-01-15T04:20:02.803542Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(ylinear):\n",
    "    return nd.maximum(ylinear, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.822109Z",
     "start_time": "2018-01-15T04:20:02.814598Z"
    }
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, 784))\n",
    "    h1 = nd.dot(X, W1) + b1\n",
    "    h1_activate = relu(h1)\n",
    "    output = nd.dot(h1_activate, W2) + b2\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.832500Z",
     "start_time": "2018-01-15T04:20:02.828394Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.847375Z",
     "start_time": "2018-01-15T04:20:02.837666Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad\n",
    "        \n",
    "# momentum\n",
    "def momentum_sgd(params, vs, lr, mom):\n",
    "    for (param, v) in zip(params, vs):\n",
    "        v[:] = mom * v + lr * param.grad / batch_size\n",
    "        param[:] = param -  v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:02.863395Z",
     "start_time": "2018-01-15T04:20:02.853613Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(img_iter, net):\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "    for i, (data, label) in enumerate(img_iter):\n",
    "        output = net(data)\n",
    "        prediction = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(prediction == label) \n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:20:04.573681Z",
     "start_time": "2018-01-15T04:20:02.869588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T05:00:05.940027Z",
     "start_time": "2018-01-15T04:20:04.578335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg train loss 0.648661682765, Train acc 0.827833, Test acc 0.8307\n",
      "Epoch 1, Avg train loss 0.460383709971, Train acc 0.8517, Test acc 0.8523\n",
      "Epoch 2, Avg train loss 0.41622438345, Train acc 0.861067, Test acc 0.8611\n",
      "Epoch 3, Avg train loss 0.392146313858, Train acc 0.871, Test acc 0.8699\n",
      "Epoch 4, Avg train loss 0.370963144207, Train acc 0.874267, Test acc 0.8688\n",
      "Epoch 5, Avg train loss 0.357209223207, Train acc 0.87885, Test acc 0.8724\n",
      "Epoch 6, Avg train loss 0.346458313719, Train acc 0.877867, Test acc 0.8744\n",
      "Epoch 7, Avg train loss 0.333545173422, Train acc 0.8841, Test acc 0.8745\n",
      "Epoch 8, Avg train loss 0.325957826233, Train acc 0.890733, Test acc 0.8798\n",
      "Epoch 9, Avg train loss 0.316037964598, Train acc 0.892517, Test acc 0.8798\n",
      "Epoch 10, Avg train loss 0.309185288302, Train acc 0.895133, Test acc 0.8838\n",
      "Epoch 11, Avg train loss 0.301318749475, Train acc 0.89905, Test acc 0.8828\n",
      "Epoch 12, Avg train loss 0.295224978733, Train acc 0.8964, Test acc 0.883\n",
      "Epoch 13, Avg train loss 0.288825058206, Train acc 0.90065, Test acc 0.8893\n",
      "Epoch 14, Avg train loss 0.283057550907, Train acc 0.898183, Test acc 0.8801\n",
      "Epoch 15, Avg train loss 0.278223341179, Train acc 0.903967, Test acc 0.8837\n",
      "Epoch 16, Avg train loss 0.275221613757, Train acc 0.907967, Test acc 0.8888\n",
      "Epoch 17, Avg train loss 0.268712975311, Train acc 0.904583, Test acc 0.8846\n",
      "Epoch 18, Avg train loss 0.262782518101, Train acc 0.91105, Test acc 0.8912\n",
      "Epoch 19, Avg train loss 0.259495623016, Train acc 0.909867, Test acc 0.8872\n",
      "Epoch 20, Avg train loss 0.253721381124, Train acc 0.91105, Test acc 0.8875\n",
      "Epoch 21, Avg train loss 0.250757078234, Train acc 0.914117, Test acc 0.8908\n",
      "Epoch 22, Avg train loss 0.24700150067, Train acc 0.917967, Test acc 0.8934\n",
      "Epoch 23, Avg train loss 0.24279944609, Train acc 0.9179, Test acc 0.8905\n",
      "Epoch 24, Avg train loss 0.239598128541, Train acc 0.919017, Test acc 0.8917\n",
      "Epoch 25, Avg train loss 0.235195264721, Train acc 0.920033, Test acc 0.893\n",
      "Epoch 26, Avg train loss 0.230528116973, Train acc 0.912433, Test acc 0.8837\n",
      "Epoch 27, Avg train loss 0.226960246547, Train acc 0.922933, Test acc 0.8931\n",
      "Epoch 28, Avg train loss 0.225052503141, Train acc 0.922467, Test acc 0.8908\n",
      "Epoch 29, Avg train loss 0.222208309539, Train acc 0.927167, Test acc 0.8934\n",
      "Epoch 30, Avg train loss 0.218607503589, Train acc 0.926583, Test acc 0.8924\n",
      "Epoch 31, Avg train loss 0.215718822575, Train acc 0.925267, Test acc 0.8909\n",
      "Epoch 32, Avg train loss 0.211643104251, Train acc 0.9252, Test acc 0.8927\n",
      "Epoch 33, Avg train loss 0.20960559322, Train acc 0.933033, Test acc 0.8953\n",
      "Epoch 34, Avg train loss 0.206258890788, Train acc 0.931717, Test acc 0.8952\n",
      "Epoch 35, Avg train loss 0.203690156714, Train acc 0.934467, Test acc 0.8966\n",
      "Epoch 36, Avg train loss 0.200744695791, Train acc 0.9304, Test acc 0.8921\n",
      "Epoch 37, Avg train loss 0.197472453992, Train acc 0.934367, Test acc 0.8962\n",
      "Epoch 38, Avg train loss 0.195488322878, Train acc 0.934133, Test acc 0.8976\n",
      "Epoch 39, Avg train loss 0.193052763573, Train acc 0.936233, Test acc 0.8948\n",
      "Epoch 40, Avg train loss 0.189224147892, Train acc 0.9352, Test acc 0.8966\n",
      "Epoch 41, Avg train loss 0.189311475086, Train acc 0.9373, Test acc 0.8979\n",
      "Epoch 42, Avg train loss 0.183669364023, Train acc 0.941567, Test acc 0.8963\n",
      "Epoch 43, Avg train loss 0.182151972739, Train acc 0.94425, Test acc 0.8978\n",
      "Epoch 44, Avg train loss 0.180414991204, Train acc 0.93945, Test acc 0.8983\n",
      "Epoch 45, Avg train loss 0.179341220411, Train acc 0.943767, Test acc 0.898\n",
      "Epoch 46, Avg train loss 0.175522527186, Train acc 0.931617, Test acc 0.8916\n",
      "Epoch 47, Avg train loss 0.173218133434, Train acc 0.94125, Test acc 0.896\n",
      "Epoch 48, Avg train loss 0.171742195082, Train acc 0.94165, Test acc 0.8941\n",
      "Epoch 49, Avg train loss 0.170634368706, Train acc 0.938183, Test acc 0.8917\n",
      "Epoch 50, Avg train loss 0.16614285984, Train acc 0.942283, Test acc 0.8923\n",
      "Epoch 51, Avg train loss 0.164019392856, Train acc 0.9459, Test acc 0.8975\n",
      "Epoch 52, Avg train loss 0.162723959891, Train acc 0.94885, Test acc 0.8984\n",
      "Epoch 53, Avg train loss 0.161334523153, Train acc 0.934617, Test acc 0.8849\n",
      "Epoch 54, Avg train loss 0.159533405161, Train acc 0.944383, Test acc 0.8943\n",
      "Epoch 55, Avg train loss 0.157399750868, Train acc 0.947817, Test acc 0.9009\n",
      "Epoch 56, Avg train loss 0.155619328801, Train acc 0.95185, Test acc 0.9008\n",
      "Epoch 57, Avg train loss 0.152856205765, Train acc 0.953533, Test acc 0.8944\n",
      "Epoch 58, Avg train loss 0.152338594596, Train acc 0.942017, Test acc 0.8903\n",
      "Epoch 59, Avg train loss 0.151033430425, Train acc 0.945683, Test acc 0.8931\n",
      "Epoch 60, Avg train loss 0.147700815193, Train acc 0.952767, Test acc 0.8966\n",
      "Epoch 61, Avg train loss 0.146705542803, Train acc 0.945833, Test acc 0.8924\n",
      "Epoch 62, Avg train loss 0.14328590289, Train acc 0.95635, Test acc 0.8972\n",
      "Epoch 63, Avg train loss 0.14357638731, Train acc 0.95605, Test acc 0.8982\n",
      "Epoch 64, Avg train loss 0.139252892868, Train acc 0.953, Test acc 0.8987\n",
      "Epoch 65, Avg train loss 0.139818780446, Train acc 0.959883, Test acc 0.8996\n",
      "Epoch 66, Avg train loss 0.137950086745, Train acc 0.95925, Test acc 0.8989\n",
      "Epoch 67, Avg train loss 0.135359215101, Train acc 0.950083, Test acc 0.8952\n",
      "Epoch 68, Avg train loss 0.132930687308, Train acc 0.951683, Test acc 0.8935\n",
      "Epoch 69, Avg train loss 0.131536135737, Train acc 0.953983, Test acc 0.8953\n",
      "Epoch 70, Avg train loss 0.132213632592, Train acc 0.9585, Test acc 0.8979\n",
      "Epoch 71, Avg train loss 0.129013268677, Train acc 0.961317, Test acc 0.8957\n",
      "Epoch 72, Avg train loss 0.128487354684, Train acc 0.962517, Test acc 0.9005\n",
      "Epoch 73, Avg train loss 0.125755617984, Train acc 0.9466, Test acc 0.8893\n",
      "Epoch 74, Avg train loss 0.124765196482, Train acc 0.965867, Test acc 0.9006\n",
      "Epoch 75, Avg train loss 0.123880747962, Train acc 0.960467, Test acc 0.8976\n",
      "Epoch 76, Avg train loss 0.122236600033, Train acc 0.96435, Test acc 0.9017\n",
      "Epoch 77, Avg train loss 0.119871656195, Train acc 0.94305, Test acc 0.8804\n",
      "Epoch 78, Avg train loss 0.118262939056, Train acc 0.964883, Test acc 0.8995\n",
      "Epoch 79, Avg train loss 0.116873543255, Train acc 0.96505, Test acc 0.8989\n",
      "Epoch 80, Avg train loss 0.116220727567, Train acc 0.9685, Test acc 0.899\n",
      "Epoch 81, Avg train loss 0.114027192426, Train acc 0.965017, Test acc 0.8979\n",
      "Epoch 82, Avg train loss 0.112655212879, Train acc 0.96735, Test acc 0.8994\n",
      "Epoch 83, Avg train loss 0.110734817314, Train acc 0.96855, Test acc 0.8996\n",
      "Epoch 84, Avg train loss 0.109466675782, Train acc 0.962167, Test acc 0.8963\n",
      "Epoch 85, Avg train loss 0.10757831068, Train acc 0.96635, Test acc 0.8959\n",
      "Epoch 86, Avg train loss 0.108125986052, Train acc 0.967567, Test acc 0.8965\n",
      "Epoch 87, Avg train loss 0.105805752603, Train acc 0.962933, Test acc 0.8906\n",
      "Epoch 88, Avg train loss 0.103169503752, Train acc 0.96975, Test acc 0.8984\n",
      "Epoch 89, Avg train loss 0.106017580903, Train acc 0.967417, Test acc 0.8952\n",
      "Epoch 90, Avg train loss 0.101558189758, Train acc 0.966017, Test acc 0.9\n",
      "Epoch 91, Avg train loss 0.100574479946, Train acc 0.9589, Test acc 0.8938\n",
      "Epoch 92, Avg train loss 0.0991896714369, Train acc 0.9728, Test acc 0.9015\n",
      "Epoch 93, Avg train loss 0.0989958113591, Train acc 0.968533, Test acc 0.896\n",
      "Epoch 94, Avg train loss 0.0967824335217, Train acc 0.968617, Test acc 0.8974\n",
      "Epoch 95, Avg train loss 0.0964787947973, Train acc 0.9734, Test acc 0.9006\n",
      "Epoch 96, Avg train loss 0.096231834201, Train acc 0.969217, Test acc 0.8955\n",
      "Epoch 97, Avg train loss 0.0913706257224, Train acc 0.9712, Test acc 0.8964\n",
      "Epoch 98, Avg train loss 0.0917534449657, Train acc 0.972717, Test acc 0.9006\n",
      "Epoch 99, Avg train loss 0.0917938803832, Train acc 0.97105, Test acc 0.8962\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0.0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate / batch_size)\n",
    "        # momentum_sgd(params, vs, learning_rate, mom=0.9)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    train_acc = evaluate_accuracy(train_data, net)\n",
    "    test_acc = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %s, Avg train loss %s, Train acc %s, Test acc %s\" \n",
    "          % (epoch, cumulative_loss / num_examples, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
