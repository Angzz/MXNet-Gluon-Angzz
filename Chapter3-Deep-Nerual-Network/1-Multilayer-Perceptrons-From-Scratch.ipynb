{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零重新看待多层感知机\n",
    "\n",
    "多层感知机的基本原理：\n",
    "\n",
    "$$h_1 = \\phi(W_1\\boldsymbol{x} + b_1)$$\n",
    "\n",
    "$$h_2 = \\phi(W_2\\boldsymbol{h_1} + b_2)$$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$h_n = \\phi(W_n\\boldsymbol{h_{n-1}} + b_n)$$\n",
    "\n",
    "$$\\hat{y} = \\mbox{softmax}(W_y \\boldsymbol{h}_n + b_y)$$\n",
    "\n",
    "## <font color=\"blue\">**使用softmax和交叉熵混合的原因**\n",
    "\n",
    "首先我们回顾softmax的公式：\n",
    "\n",
    "$$\\hat y_j = \\frac{e^{z_j}}{\\sum_{i=1}^{n} e^{z_i}}$$\n",
    "\n",
    "其中$\\hat y_j$是$cross\\_entropy$函数的输入$yhat$的第$j-th$个元素，$z_j$是softmax函数中输入$y\\_linear$的第$j-th$个元素。\n",
    "\n",
    "有种情况是当我们的$z_j$很大时，$e^{z_j}$的结果会很大从而导致上溢，这时候分子会接近``inf``从而我们的结果$\\hat y_j$就会不确定(``inf``, ``nan``, 0)，这时候就会产生数值不稳定性，因此我们先对每个$z_j$首先减去其最大值，这样就不会出现正无穷的情况，我们可以证明，<font color=\"red\">**减去最大值并不影响$softmax$结果的输出</font>。**\n",
    "\n",
    "进一步我们考虑到，在上面减去最大值后,$z_j$同样可能会趋近于负无穷，那么此时$e^{z_j}$就会趋近于0，此时同样产生数值的不稳定性，例如当$\\hat y_j$趋近于0时，接下来的交叉熵步骤$log(\\hat y_j)$就会趋近于负无穷，那么我们利用这样的负无穷的loss反向传播时，其结果将会非常可怕。\n",
    "\n",
    "因此我们可以巧妙的将$softmax$和$cross\\_entropy$的步骤结合起来，来避开先求导然后求对数的过程，这样可以避开这样的数值不稳定性的发生，公式推导如下：\n",
    "\n",
    "$$\\text{log}{(\\hat y_j)} = \\text{log}\\left( \\frac{e^{z_j}}{\\sum_{i=1}^{n} e^{z_i}}\\right) = \\text{log}{(e^{z_j})}-\\text{log}{\\left( \\sum_{i=1}^{n} e^{z_i} \\right)} = z_j -\\text{log}{\\left( \\sum_{i=1}^{n} e^{z_i} \\right)}$$\n",
    "\n",
    "因此我们正确的做法是，写出$softmax$函数只用于计算其概率，但我们不将$softmax$输出的概率传入我们的损失函数中，而是直接将$softmax$的输入$y\n",
    "\\_linear$传入损失函数中，这样可以使得数值更具稳定性。\n",
    "\n",
    "**下面我们来实现本章的算法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:48:57.923022Z",
     "start_time": "2018-01-17T08:48:57.394808Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:48:57.932362Z",
     "start_time": "2018-01-17T08:48:57.928288Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:48:58.433105Z",
     "start_time": "2018-01-17T08:48:57.937487Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data, test_data = utils.load_dataset(batch_size, data_type='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:48:58.464826Z",
     "start_time": "2018-01-17T08:48:58.438677Z"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = 60000\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_hidden = 256\n",
    "weight_scale = .01\n",
    "\n",
    "W1 = nd.random.normal(shape=(num_inputs, num_hidden), scale=weight_scale)\n",
    "b1 = nd.random.normal(shape=(num_hidden,))\n",
    "\n",
    "W2 = nd.random.normal(shape=(num_hidden, num_hidden), scale=weight_scale)\n",
    "b2 = nd.random.normal(shape=(num_hidden,))\n",
    "\n",
    "W3 = nd.random.normal(shape=(num_hidden, num_outputs), scale=weight_scale)\n",
    "b3 = nd.random.normal(shape=(num_outputs,))\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:48:58.568670Z",
     "start_time": "2018-01-17T08:48:58.474121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_data:\n",
    "    print(data.shape)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义$softmax$交叉熵损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:48:58.584752Z",
     "start_time": "2018-01-17T08:48:58.573476Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    yexp = nd.exp(y_linear - nd.max(y_linear))\n",
    "    return yexp / nd.nansum(yexp, axis=1, exclude=True)\n",
    "\n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return - nd.nansum(y * nd.log(softmax(yhat)), axis=1, exclude=True)\n",
    "\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:48:58.605932Z",
     "start_time": "2018-01-17T08:48:58.590679Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(y_linear):\n",
    "    return nd.maximum(y_linear, nd.zeros_like(y_linear))\n",
    "    \n",
    "def net(X):\n",
    "    X = X.reshape((-1, 784))\n",
    "    # first layer\n",
    "    h1 = nd.dot(X, W1) + b1\n",
    "    h1_relu = relu(h1)\n",
    "    \n",
    "    #second layer \n",
    "    h2 = nd.dot(h1_relu, W2) + b2\n",
    "    h2_relu = relu(h2)\n",
    "    \n",
    "    #output layer \n",
    "    output = nd.dot(h2_relu, W3) + b3\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T12:24:37.275665Z",
     "start_time": "2018-01-16T12:24:37.241374Z"
    }
   },
   "source": [
    "## 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-17T08:48:57.441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10441667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.evaluate_accuracy_scratch(train_data, net, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-17T08:48:57.444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss 3.00007872289, Train acc 0.905467, Test acc 0.9012.\n",
      "Epoch 1, Train loss 2.71165451838, Train acc 0.949433, Test acc 0.9423.\n",
      "Epoch 2, Train loss 2.6723713829, Train acc 0.955033, Test acc 0.9503.\n",
      "Epoch 3, Train loss 2.65166878611, Train acc 0.96935, Test acc 0.9573.\n",
      "Epoch 4, Train loss 2.63810422694, Train acc 0.969033, Test acc 0.9571.\n",
      "Epoch 5, Train loss 2.62684993515, Train acc 0.978883, Test acc 0.9644.\n",
      "Epoch 6, Train loss 2.61908394419, Train acc 0.98335, Test acc 0.9708.\n",
      "Epoch 7, Train loss 2.6138356458, Train acc 0.984983, Test acc 0.9706.\n",
      "Epoch 8, Train loss 2.60796582514, Train acc 0.992133, Test acc 0.9751.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = .0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)   \n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "    train_acc = utils.evaluate_accuracy_scratch(train_data, net, ctx)\n",
    "    test_acc = utils.evaluate_accuracy_scratch(test_data, net, ctx)\n",
    "    \n",
    "    print(\"Epoch %s, Train loss %s, Train acc %s, Test acc %s.\" \n",
    "          % (epoch, cumulative_loss / num_examples, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
