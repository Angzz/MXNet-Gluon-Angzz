{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T08:53:55.613237Z",
     "start_time": "2018-01-19T08:53:55.485926Z"
    }
   },
   "source": [
    "# 从0开始卷积神经网络\n",
    "\n",
    "卷积层中的每个数据层都代表一个3维输入张量$(in\\_channels \\times height \\times weight)$，此外，我们想要经过卷积层的输出通道数设为$ouput\\_channels$，因此，每个卷积层我们都用一个四维的张量来表示，即：\n",
    "\n",
    "$$(output\\_channels \\times input\\_channels \\times height \\times width)$$\n",
    "\n",
    "这些四维张量组成了一个权重矩阵$W$，其中权重矩阵的各个参数：\n",
    "* $output\\_channels$ : 表示我们想要该卷积层中有多少个输出通道\n",
    "* $in\\_channels$ : 表示该层的输入通道数，也即上一层的输出通道数\n",
    "* $height$ : 卷积核(Kernel)的高\n",
    "* $width$ : 卷积核(kernel)的宽\n",
    "\n",
    "而对应的在卷积神经网络中我们的输入数据的格式均为四维张量：\n",
    "\n",
    "$$(batch \\times channel \\times height \\times width)$$\n",
    "\n",
    "<img src=\"http://zh.gluon.ai/_images/no_padding_no_strides.gif\">\n",
    "\n",
    "<font color=\"red\">一定要记住卷积层中的**权重**的张量格式以及我们**输入输出**数据的格式，这样就很好办了</font>\n",
    "\n",
    "下面我们使用卷积神经网络实现MNIST分类，**这是我们最后一次使用MNIST数据集**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:36.694736Z",
     "start_time": "2018-02-02T03:38:36.240512Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "import utils\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.386013Z",
     "start_time": "2018-02-02T03:38:36.699187Z"
    }
   },
   "outputs": [],
   "source": [
    "num_outputs = 10\n",
    "num_inputs = 784\n",
    "batch_size = 128\n",
    "\n",
    "train_data, test_data = utils.load_dataset(batch_size, data_type='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.492489Z",
     "start_time": "2018-02-02T03:38:37.391047Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_scale = .01\n",
    "num_filter_conv1 = 20\n",
    "num_filter_conv2 = 50\n",
    "\n",
    "############# Conv layer ###############\n",
    "W1 = nd.random.normal(shape=(num_filter_conv1, 1, 3, 3), scale=weight_scale, ctx=ctx)\n",
    "b1 = nd.random.normal(shape=num_filter_conv1, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "W2 = nd.random.normal(shape=(num_filter_conv2, num_filter_conv1, 5, 5), scale=weight_scale, ctx=ctx)\n",
    "b2 = nd.random.normal(shape=num_filter_conv2, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "############# FC layer ###############\n",
    "\n",
    "num_input_fc1 = 800\n",
    "num_output_fc1 = 128\n",
    "\n",
    "W3 = nd.random.normal(shape=(num_input_fc1, num_output_fc1), scale=weight_scale, ctx=ctx)\n",
    "b3 = nd.random.normal(shape=num_output_fc1, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "W4 = nd.random.normal(shape=(num_output_fc1, num_outputs), scale=weight_scale, ctx=ctx)\n",
    "b4 = nd.random.normal(shape=num_outputs, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "############# attach grad ###############\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维度测试\n",
    "\n",
    "**<font color=\"red\">使用卷积神经网络一定要进行维度测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.494939Z",
     "start_time": "2018-02-02T03:29:00.697Z"
    }
   },
   "outputs": [],
   "source": [
    "for data, _ in train_data:\n",
    "    data = data.as_in_context(ctx)\n",
    "    break\n",
    "\n",
    "print(\"data shape : \", data.shape)\n",
    "    \n",
    "conv1 = nd.Convolution(data=data, weight=W1, bias=b1, kernel=W1.shape[2:], \n",
    "                       stride=(1,1), num_filter=W1.shape[0])\n",
    "print(\"conv1 shape : \", conv1.shape)\n",
    "\n",
    "pool1 = nd.Pooling(data=conv1, kernel=(2,2), pool_type='max', stride=(2,2))\n",
    "print(\"pool1 shape : \", pool1.shape)\n",
    "\n",
    "conv2 = nd.Convolution(data=pool1, weight=W2, bias=b2, kernel=W2.shape[2:], \n",
    "                       stride=(1,1), num_filter=W2.shape[0])\n",
    "print(\"conv1 shape : \", conv2.shape)\n",
    "\n",
    "pool2 = nd.Pooling(data=conv2, kernel=(2,2), pool_type='max', stride=(2,2))\n",
    "print(\"pool2 shape : \", pool2.shape)\n",
    "\n",
    "############# 由此，我们可以确定我们全连接层要Flatten的input_units输入为800 (50*4*4) #############\n",
    "\n",
    "flatten = nd.Flatten(pool2)\n",
    "print(\"flatten shape : \", flatten.shape)\n",
    "\n",
    "fc1 = nd.dot(flatten, W3) + b3\n",
    "print(\"fc1 shape : \", fc1.shape)\n",
    "\n",
    "fc2 = nd.dot(fc1, W4) + b4\n",
    "print(\"fc2 shape : \", fc2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.498161Z",
     "start_time": "2018-02-02T03:29:00.702Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.500277Z",
     "start_time": "2018-02-02T03:29:00.707Z"
    }
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    \n",
    "    # Conv1 Layer \n",
    "    conv1 = nd.Convolution(data=X, weight=W1, bias=b1, kernel=W1.shape[2:], \n",
    "                       stride=(1,1), num_filter=W1.shape[0])\n",
    "    pool1 = nd.Pooling(data=conv1, kernel=(2,2), pool_type=\"max\", stride=(2,2))\n",
    "    relu1 = relu(pool1)\n",
    "    \n",
    "    # Conv2 Layer \n",
    "    conv2 = nd.Convolution(data=relu1, weight=W2, bias=b2, kernel=W2.shape[2:],\n",
    "                          stride=(1,1), num_filter=W2.shape[0])\n",
    "    pool2 = nd.Pooling(data=conv2, kernel=(2,2), pool_type=\"max\", stride=(2,2))\n",
    "    relu2 = relu(pool2)\n",
    "    \n",
    "    # fc1 Layer \n",
    "    flatten = nd.Flatten(relu2)\n",
    "    fc1 = nd.dot(flatten, W3) + b3\n",
    "    relu3 = relu(fc1)\n",
    "    \n",
    "    # fc2 Layer\n",
    "    fc2 = nd.dot(relu3, W4) + b4\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.503239Z",
     "start_time": "2018-02-02T03:29:00.713Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(ylinear):\n",
    "    yexp = nd.exp(ylinear - nd.max(ylinear).asscalar())\n",
    "    partition = yexp / nd.sum(yexp, axis=1).reshape((-1, 1))\n",
    "    return partition\n",
    " \n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return -nd.sum(y * nd.log(softmax(yhat)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.504166Z",
     "start_time": "2018-02-02T03:29:00.715Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.508880Z",
     "start_time": "2018-02-02T03:29:00.722Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(img_iter, net, ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(img_iter):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        preds = nd.argmax(net(data), axis=1)\n",
    "        acc.update(preds=preds, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.509941Z",
     "start_time": "2018-02-02T03:29:00.724Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_accuracy(train_data, net, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T03:38:37.510853Z",
     "start_time": "2018-02-02T03:29:00.727Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "num_examples = 60000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = .0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    train_acc = evaluate_accuracy(train_data, net, ctx)\n",
    "    test_acc = evaluate_accuracy(test_data, net, ctx)\n",
    "    print(\"Epoch %s, Train Avg Loss %s, Train acc %s, Test acc %s\"\n",
    "         % (epoch, cumulative_loss / num_examples, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
