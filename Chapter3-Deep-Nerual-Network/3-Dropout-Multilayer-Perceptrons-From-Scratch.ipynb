{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "神经网络中如果产生过拟合，说明每一层过分依赖于其前一层中特征的确切配置，为了防止神经网络过度依赖于之前层的特征的激活路径，我们可以以$0.5\\%$的概率随机丢弃掉某一隐层中的部分节点，因此给定一个具有$n$个节点的网络，我们大致相当于从一个部分节点被丢弃的$2^n$种网络中随机均匀采样。\n",
    "\n",
    "<img src=\"http://www.angzz.top:8888/notebooks/Open-Source-Project/mxnet-the-straight-dope/img/dropout.png\" width=\"300\">\n",
    "\n",
    "$drpoout$起作用的直观解释就是：因为每次都要随机选择丢弃的节点，因此每一层中的表示都不能依赖于前一层中节点的确切值。\n",
    "\n",
    "随机失活在约束网络复杂度同时，还是一种针对深度模型的高效集成学习(ensemble learning)的方法。传统神经网络中，由于神经元间的互联，对于某单个神经元来说，其反向传导来的梯度信息同时也受到其他神经元的影响，可谓“牵一发而动全身”。这就是所谓的“复杂协同适应”效应。随机失活的提出正 是一定程度上缓解了神经元之间复杂的协同适应，降低了神经元间依赖，避免了网络过拟合的发生。其原理非常简单:**对于某层的每个神经元，在训练阶段 均以概率$p$随机将该神经元权重置(故被称作“随机失活”)，测试阶段所有神经元均呈激活态，但其权重需乘$(1-p)$以保证训练和测试阶段各自权重拥有相同的期望。**\n",
    "\n",
    "[**浅析Dropout层中为什么在训练阶段dropout后在测试阶段要进行rescale**](https://www.zhihu.com/question/61751133)\n",
    "\n",
    "**<font color=\"red\">分析如下</font>**：当模型使用了dropout layer以后，我们以概率$p$丢弃某一层中的部分神经元，那么训练时候只有占比为$1-p$的隐藏层单元参与训练。而在预测的时候，如果我们需要所有的隐藏层单元都要参与进来，那么其得到的结果相比训练时平均要大$\\frac{1}{1-p}$，为了避免这种情况，就需要测试的时候将输出结果乘以$1-p$使得下一层的输入规模(**即期望**)保持不变。\n",
    "\n",
    "而利用反向随机失活(inverted dropout)，我们可以在训练的时候直接将dropout后留下的权重扩大$\\frac{1}{1-p}$倍，这样就可以使得结果的scale保持不变，而在预测的时候就不需要做额外的操作了。这样更方便一些。\n",
    "\n",
    "**<font color=\"red\">另一种理解方式</font>**：假设我们设置dropout probability为p，那么该层大约有比例为p的单元会被丢弃，因为每个神经元是否被丢弃就是一次伯努利实验，因此若该层有n个神经元，则该层的dropout概率相当于n重伯努利实验，其服从伯努利分布，而伯努利分布的期望就是np。\n",
    "\n",
    "又因为有$z^l = w^la^{l-1} + b^l$，那么当$l-1$层有比例为p的单元被丢弃后，$a^{l-1}$的规模会变为原来的1-p倍，因此为了保证$l$层的输出$z^l$的期望不变，所以要在$a^{l-1}$与dropout矩阵乘积后，要除以$1-p$。\n",
    "\n",
    "**<font color=\"red\">第三种理解方式</font>**：10个人拉一个10吨车，第一次（训练时），只有5个人出力（有p=0.5的人被dropout了），那么这5个人每个人出力拉2吨。第二次（预测时），10个人都被要求出力，这次每个人出的力就是2*（1-0.5）=1吨了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:49.889352Z",
     "start_time": "2018-01-17T08:58:48.269731Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "mx.random.seed(1)\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:49.903060Z",
     "start_time": "2018-01-17T08:58:49.897120Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.221844Z",
     "start_time": "2018-01-17T08:58:49.909394Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data, test_data = utils.load_dataset(batch_size, data_type='fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.291704Z",
     "start_time": "2018-01-17T08:58:51.229718Z"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_hidden = 256\n",
    "num_outputs = 10\n",
    "num_examples = 60000\n",
    "weight_scale = 0.01\n",
    "\n",
    "W1 = nd.random.normal(shape=(num_inputs, num_hidden), scale=weight_scale)\n",
    "b1 = nd.random.normal(shape=(num_hidden, ), scale=weight_scale)\n",
    "\n",
    "W2 = nd.random.normal(shape=(num_hidden, num_hidden), scale=weight_scale)\n",
    "b2 = nd.random.normal(shape=(num_hidden, ), scale=weight_scale)\n",
    "\n",
    "W3 = nd.random.normal(shape=(num_hidden, num_outputs), scale=weight_scale)\n",
    "b3 = nd.random.normal(shape=(num_outputs, ), scale=weight_scale)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.312699Z",
     "start_time": "2018-01-17T08:58:51.295510Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.336468Z",
     "start_time": "2018-01-17T08:58:51.318377Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob = 0.0):\n",
    "    keep_prob = 1 - drop_prob\n",
    "    mask = nd.random.uniform(0.0, 1.0, X.shape) > drop_prob\n",
    "    if keep_prob == 0.0:\n",
    "        scale = 1\n",
    "    else:\n",
    "         # 保证 E[dropout(X)] == X\n",
    "        scale = 1 / (1 - drop_prob)\n",
    "    return X * mask * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.365561Z",
     "start_time": "2018-01-17T08:58:51.344919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  1.   2.   3.   4.]\n",
       " [  5.   6.   7.   8.]\n",
       " [  9.  10.  11.  12.]\n",
       " [ 13.  14.  15.  16.]\n",
       " [ 17.  18.  19.  20.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nd.arange(20).reshape((5, 4)) + 1\n",
    "dropout(a, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.387428Z",
     "start_time": "2018-01-17T08:58:51.374180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  0.   4.   6.   0.]\n",
       " [  0.   0.  14.  16.]\n",
       " [ 18.   0.   0.   0.]\n",
       " [ 26.   0.  30.  32.]\n",
       " [  0.   0.   0.  40.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(a, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.407591Z",
     "start_time": "2018-01-17T08:58:51.394166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(a, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax_cross_entropy (维持数值稳定性)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.429462Z",
     "start_time": "2018-01-17T08:58:51.414897Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(ylinear):\n",
    "    yexp = nd.exp(ylinear - nd.max(ylinear))\n",
    "    partition = yexp / nd.nansum(yexp, axis=1, exclude=True)\n",
    "    return partition\n",
    "\n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return -nd.nansum(y * nd.log(softmax(yhat)), axis=1, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T08:58:51.464279Z",
     "start_time": "2018-01-17T08:58:51.437817Z"
    }
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, 784))\n",
    "    h1 = nd.dot(X, W1) + b1\n",
    "    h1 = relu(h1)\n",
    "    h1 = dropout(h1, 0.2)\n",
    "    \n",
    "    h2 = nd.dot(h1, W2) + b2\n",
    "    h2 = relu(h2)\n",
    "    h2 = dropout(h2, 0.5)\n",
    "    \n",
    "    h3 = nd.dot(h2, W3) + b3\n",
    "    return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-17T08:58:48.277Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-17T08:58:48.281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss 3.82514266561, Train acc 0.64995, Test acc 0.6511.\n",
      "Epoch 1, Train loss 3.22903592224, Train acc 0.683583, Test acc 0.6842.\n",
      "Epoch 2, Train loss 3.11905423635, Train acc 0.764083, Test acc 0.7641.\n",
      "Epoch 3, Train loss 3.06101858927, Train acc 0.80915, Test acc 0.8086.\n",
      "Epoch 4, Train loss 3.02602425537, Train acc 0.839517, Test acc 0.8334.\n",
      "Epoch 5, Train loss 2.99819885406, Train acc 0.8583, Test acc 0.8477.\n",
      "Epoch 6, Train loss 2.97827471339, Train acc 0.85755, Test acc 0.848.\n",
      "Epoch 7, Train loss 2.96405434977, Train acc 0.862467, Test acc 0.854.\n",
      "Epoch 8, Train loss 2.9488596817, Train acc 0.872, Test acc 0.8635.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = .0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "    train_acc = utils.evaluate_accuracy_scratch(train_data, net, ctx)\n",
    "    test_acc = utils.evaluate_accuracy_scratch(test_data, net, ctx)\n",
    "    print(\"Epoch %s, Train loss %s, Train acc %s, Test acc %s.\" \n",
    "          % (epoch, cumulative_loss / num_examples, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
