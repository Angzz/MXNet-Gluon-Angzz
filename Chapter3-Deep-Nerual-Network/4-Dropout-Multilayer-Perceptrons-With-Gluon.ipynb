{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout ``gluon``\n",
    "\n",
    "Dropout是一种特殊的层，因为它在训练和测试时候的表现不同，``gluon``可以捕获什么时候record计算图而什么时候不去record计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:28.258340Z",
     "start_time": "2018-01-17T09:04:27.194967Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:28.271358Z",
     "start_time": "2018-01-17T09:04:28.264881Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.281262Z",
     "start_time": "2018-01-17T09:04:28.278197Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data, test_data = utils.load_dataset(batch_size, data_type='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.326225Z",
     "start_time": "2018-01-17T09:04:29.287733Z"
    }
   },
   "outputs": [],
   "source": [
    "num_outputs = 10\n",
    "num_inputs = 784\n",
    "num_hidden = 256\n",
    "num_examples = 60000\n",
    "\n",
    "drop_prob1 = 0.2\n",
    "drop_prob2 = 0.5\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(drop_prob1))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(drop_prob2))\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用``train_mode``和``predict_mode``\n",
    "\n",
    "我们可以看到，同一个值经过训练一次后的网络后的值竟然没有改变，这是因为``mxnet``此时知道我们处于``predict_mode``，它不会去训练网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.548838Z",
     "start_time": "2018-01-17T09:04:29.334902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 28, 28)\n",
      "\n",
      "[[ 0.17661531 -0.04132318  0.09641108  0.00084138  0.01041453  0.28043959\n",
      "  -0.24985664 -0.01252007 -0.27903724 -0.16061771]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "\n",
      "[[ 0.17661531 -0.04132318  0.09641108  0.00084138  0.01041453  0.28043959\n",
      "  -0.24985664 -0.01252007 -0.27903724 -0.16061771]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for x, _ in train_data:\n",
    "    x = x.as_in_context(ctx)\n",
    "    print(x.shape)\n",
    "    break\n",
    "print(net(x[0:1]))\n",
    "print(net(x[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以显示的指定这个过程``predict_mode``，以使得``mxnet``知道我们在进行这个过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.574765Z",
     "start_time": "2018-01-17T09:04:29.557444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.17661531 -0.04132318  0.09641108  0.00084138  0.01041453  0.28043959\n",
      "  -0.24985664 -0.01252007 -0.27903724 -0.16061771]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "\n",
      "[[ 0.17661531 -0.04132318  0.09641108  0.00084138  0.01041453  0.28043959\n",
      "  -0.24985664 -0.01252007 -0.27903724 -0.16061771]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.predict_mode():\n",
    "    print(net(x[:1]))\n",
    "    print(net(x[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除非出现可怕的错误，否则你应该看到和以前一样的结果。我们也可以在``train_mode``下运行代码。这将告诉MXNet运行我们的``Block``，这会带来结果的改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.599804Z",
     "start_time": "2018-01-17T09:04:29.580662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.01276705 -0.26997188  0.06107473  0.32555851 -0.20866846  0.1562108\n",
      "  -0.13003586 -0.01198454 -0.35130394 -0.1841678 ]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "\n",
      "[[ 0.19918826 -0.05950467 -0.07602523  0.03314148  0.20677546  0.48302743\n",
      "  -0.3097747   0.16385019 -0.18783174 -0.20046568]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.train_mode():\n",
    "    print(net(x[:1]))\n",
    "    print(net(x[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过 ``is_training()`` 方法来判断到底是处在``predict_mode``还是``train_mode``，默认情况下MXNet会处在``predict_mode``，因为我们不是时时刻刻都想去训练一个模型，我们可能只是想看看预测的结果而已。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.622138Z",
     "start_time": "2018-01-17T09:04:29.606907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "with autograd.predict_mode():\n",
    "    print(autograd.is_training())\n",
    "    print(autograd.is_recording())\n",
    "    \n",
    "with autograd.train_mode():\n",
    "    print(autograd.is_training())\n",
    "    print(autograd.is_recording())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``autograd.record()``\n",
    "\n",
    "当我们训练神经网络时，我们都要``record``我们的``Block``，``record()``的目的是为了构建计算图，而``train()``的目的是为了表明我们正在训练我们的神经网络，二者并不冲突，这两个是高度相关的但是不应该混淆。例如，当我们生成对抗样本的时候(GAN中会讲到)，我们会进行``record``，但模型此时却表现为``predic_mode``。另一方面，即使我们没有``record``，我们依然想要评估模型的``train_mode``行为。\n",
    "\n",
    "因此。由于``record()``和``train_mode()``是截然不同的，我们如何避免每次训练模型时必须声明两个范围？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.646377Z",
     "start_time": "2018-01-17T09:04:29.629957Z"
    }
   },
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    with autograd.train_mode():\n",
    "        yhat = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``MXNet``的设计是：让``record()``方法默认接受一个参数``train_mode``,即当我们使用`autograd.record()``时，就相当于使用了``autograd.record(train_mode=True)``，我们可以声明``autograd.record(train_mode=False)`来改变这种行为(例如当我们使用GAN的时候)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T09:04:29.659895Z",
     "start_time": "2018-01-17T09:04:29.653605Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-17T09:04:27.265Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = .1\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-17T09:04:27.270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Moving Avg Train loss 0.342608848283, Train acc 0.9263, Test acc 0.9289.\n",
      "Epoch 1, Moving Avg Train loss 0.240437685753, Train acc 0.949483333333, Test acc 0.9485.\n",
      "Epoch 2, Moving Avg Train loss 0.180699554759, Train acc 0.960066666667, Test acc 0.9583.\n",
      "Epoch 3, Moving Avg Train loss 0.192404750316, Train acc 0.9681, Test acc 0.9655.\n",
      "Epoch 4, Moving Avg Train loss 0.130126285545, Train acc 0.972416666667, Test acc 0.9683.\n",
      "Epoch 5, Moving Avg Train loss 0.115063072135, Train acc 0.97595, Test acc 0.9709.\n",
      "Epoch 6, Moving Avg Train loss 0.0967161048028, Train acc 0.979, Test acc 0.9732.\n",
      "Epoch 7, Moving Avg Train loss 0.0963559117516, Train acc 0.981433333333, Test acc 0.9739.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "niter = 0\n",
    "moving_loss = .0\n",
    "smoothing_constant = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record(train_mode=True):\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        niter += 1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss\n",
    "        estimate_loss = moving_loss / (1 - (1-smoothing_constant)**niter)\n",
    "        \n",
    "    train_acc = utils.evaluate_accuracy_gluon(train_data, net, ctx)\n",
    "    test_acc = utils.evaluate_accuracy_gluon(test_data, net, ctx)\n",
    "    print(\"Epoch %s, Moving Avg Train loss %s, Train acc %s, Test acc %s.\" \n",
    "          % (epoch, estimate_loss, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
