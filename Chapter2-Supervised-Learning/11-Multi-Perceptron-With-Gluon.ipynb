{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用``gluon``实现多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T08:09:33.623882Z",
     "start_time": "2018-01-15T08:09:32.877322Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import image\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T08:09:34.257669Z",
     "start_time": "2018-01-15T08:09:33.629215Z"
    }
   },
   "outputs": [],
   "source": [
    "# 方法已经写进utils.py中了\n",
    "train_data, test_data = utils.load_dataset(batch_size=128, data_type='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T08:09:34.270874Z",
     "start_time": "2018-01-15T08:09:34.263201Z"
    }
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Flatten()) #reshape为2-d\n",
    "    net.add(gluon.nn.Dense(256, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T08:09:34.282676Z",
     "start_time": "2018-01-15T08:09:34.276739Z"
    }
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T08:09:34.292593Z",
     "start_time": "2018-01-15T08:09:34.288372Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T08:09:34.303469Z",
     "start_time": "2018-01-15T08:09:34.298202Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'Adam', {'wd': 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T08:09:38.137656Z",
     "start_time": "2018-01-15T08:09:34.308085Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.109"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.evaluate_accuracy_gluon(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T09:54:19.087349Z",
     "start_time": "2018-01-15T08:09:38.142755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Train loss 0.545715132014, Train acc 0.853633333333,Test acc 0.8541\n",
      "Epoch 1, Avg Train loss 0.417928052807, Train acc 0.863083333333,Test acc 0.8616\n",
      "Epoch 2, Avg Train loss 0.391252469126, Train acc 0.875933333333,Test acc 0.87\n",
      "Epoch 3, Avg Train loss 0.371207527161, Train acc 0.88025,Test acc 0.8743\n",
      "Epoch 4, Avg Train loss 0.359425410398, Train acc 0.882316666667,Test acc 0.876\n",
      "Epoch 5, Avg Train loss 0.349393465773, Train acc 0.8809,Test acc 0.8741\n",
      "Epoch 6, Avg Train loss 0.341820578543, Train acc 0.89,Test acc 0.8831\n",
      "Epoch 7, Avg Train loss 0.338259444332, Train acc 0.889966666667,Test acc 0.8783\n",
      "Epoch 8, Avg Train loss 0.336290071265, Train acc 0.88745,Test acc 0.8801\n",
      "Epoch 9, Avg Train loss 0.328158128961, Train acc 0.890566666667,Test acc 0.8809\n",
      "Epoch 10, Avg Train loss 0.325290334002, Train acc 0.886933333333,Test acc 0.8787\n",
      "Epoch 11, Avg Train loss 0.322127607886, Train acc 0.8951,Test acc 0.8854\n",
      "Epoch 12, Avg Train loss 0.321751551533, Train acc 0.88835,Test acc 0.8816\n",
      "Epoch 13, Avg Train loss 0.317471080907, Train acc 0.8915,Test acc 0.882\n",
      "Epoch 14, Avg Train loss 0.314848442777, Train acc 0.891133333333,Test acc 0.8798\n",
      "Epoch 15, Avg Train loss 0.315068226687, Train acc 0.893416666667,Test acc 0.8837\n",
      "Epoch 16, Avg Train loss 0.310396426805, Train acc 0.893083333333,Test acc 0.8809\n",
      "Epoch 17, Avg Train loss 0.308250455475, Train acc 0.8961,Test acc 0.8828\n",
      "Epoch 18, Avg Train loss 0.306926025867, Train acc 0.89225,Test acc 0.8793\n",
      "Epoch 19, Avg Train loss 0.307046925545, Train acc 0.894166666667,Test acc 0.8811\n",
      "Epoch 20, Avg Train loss 0.304164313189, Train acc 0.897333333333,Test acc 0.8824\n",
      "Epoch 21, Avg Train loss 0.302896786213, Train acc 0.8798,Test acc 0.8696\n",
      "Epoch 22, Avg Train loss 0.303207830238, Train acc 0.899083333333,Test acc 0.8844\n",
      "Epoch 23, Avg Train loss 0.300084289138, Train acc 0.9016,Test acc 0.8902\n",
      "Epoch 24, Avg Train loss 0.30031426808, Train acc 0.8983,Test acc 0.8889\n",
      "Epoch 25, Avg Train loss 0.297906510099, Train acc 0.889833333333,Test acc 0.8746\n",
      "Epoch 26, Avg Train loss 0.300309528542, Train acc 0.899733333333,Test acc 0.8842\n",
      "Epoch 27, Avg Train loss 0.295504267852, Train acc 0.89275,Test acc 0.877\n",
      "Epoch 28, Avg Train loss 0.293767783928, Train acc 0.8931,Test acc 0.8795\n",
      "Epoch 29, Avg Train loss 0.294467520555, Train acc 0.8995,Test acc 0.8859\n",
      "Epoch 30, Avg Train loss 0.29189650809, Train acc 0.896166666667,Test acc 0.8801\n",
      "Epoch 31, Avg Train loss 0.291154334418, Train acc 0.898366666667,Test acc 0.8847\n",
      "Epoch 32, Avg Train loss 0.290500334072, Train acc 0.8972,Test acc 0.8832\n",
      "Epoch 33, Avg Train loss 0.292683823999, Train acc 0.8966,Test acc 0.8845\n",
      "Epoch 34, Avg Train loss 0.290018908596, Train acc 0.904883333333,Test acc 0.8859\n",
      "Epoch 35, Avg Train loss 0.290603373973, Train acc 0.896633333333,Test acc 0.8819\n",
      "Epoch 36, Avg Train loss 0.290503389486, Train acc 0.900183333333,Test acc 0.8883\n",
      "Epoch 37, Avg Train loss 0.287194190025, Train acc 0.90285,Test acc 0.8893\n",
      "Epoch 38, Avg Train loss 0.286298663457, Train acc 0.900466666667,Test acc 0.8865\n",
      "Epoch 39, Avg Train loss 0.287188434426, Train acc 0.899566666667,Test acc 0.8818\n",
      "Epoch 40, Avg Train loss 0.28705271705, Train acc 0.89005,Test acc 0.8762\n",
      "Epoch 41, Avg Train loss 0.28776161178, Train acc 0.900183333333,Test acc 0.8854\n",
      "Epoch 42, Avg Train loss 0.284608353933, Train acc 0.9061,Test acc 0.8873\n",
      "Epoch 43, Avg Train loss 0.285334528669, Train acc 0.907416666667,Test acc 0.8904\n",
      "Epoch 44, Avg Train loss 0.283154785331, Train acc 0.9076,Test acc 0.8892\n",
      "Epoch 45, Avg Train loss 0.284180726178, Train acc 0.907366666667,Test acc 0.8912\n",
      "Epoch 46, Avg Train loss 0.283285207844, Train acc 0.90705,Test acc 0.8886\n",
      "Epoch 47, Avg Train loss 0.28037607921, Train acc 0.906383333333,Test acc 0.8909\n",
      "Epoch 48, Avg Train loss 0.28114590826, Train acc 0.899133333333,Test acc 0.883\n",
      "Epoch 49, Avg Train loss 0.280574162722, Train acc 0.89705,Test acc 0.8804\n",
      "Epoch 50, Avg Train loss 0.28253912665, Train acc 0.89855,Test acc 0.8844\n",
      "Epoch 51, Avg Train loss 0.280342432117, Train acc 0.906016666667,Test acc 0.8887\n",
      "Epoch 52, Avg Train loss 0.280731642628, Train acc 0.905716666667,Test acc 0.8886\n",
      "Epoch 53, Avg Train loss 0.280159734948, Train acc 0.905583333333,Test acc 0.8857\n",
      "Epoch 54, Avg Train loss 0.280208856678, Train acc 0.907,Test acc 0.8885\n",
      "Epoch 55, Avg Train loss 0.280498672009, Train acc 0.90695,Test acc 0.8873\n",
      "Epoch 56, Avg Train loss 0.281255027167, Train acc 0.907683333333,Test acc 0.8859\n",
      "Epoch 57, Avg Train loss 0.278999483395, Train acc 0.9038,Test acc 0.8882\n",
      "Epoch 58, Avg Train loss 0.278740587711, Train acc 0.90575,Test acc 0.8839\n",
      "Epoch 59, Avg Train loss 0.276317303403, Train acc 0.90185,Test acc 0.8807\n",
      "Epoch 60, Avg Train loss 0.281898108737, Train acc 0.903966666667,Test acc 0.8829\n",
      "Epoch 61, Avg Train loss 0.27695376447, Train acc 0.9119,Test acc 0.8916\n",
      "Epoch 62, Avg Train loss 0.27905636123, Train acc 0.9015,Test acc 0.8832\n",
      "Epoch 63, Avg Train loss 0.278836710707, Train acc 0.912,Test acc 0.8931\n",
      "Epoch 64, Avg Train loss 0.278948759778, Train acc 0.902966666667,Test acc 0.8825\n",
      "Epoch 65, Avg Train loss 0.278840119902, Train acc 0.90825,Test acc 0.8904\n",
      "Epoch 66, Avg Train loss 0.276519832261, Train acc 0.9069,Test acc 0.8885\n",
      "Epoch 67, Avg Train loss 0.275088828119, Train acc 0.904733333333,Test acc 0.8873\n",
      "Epoch 68, Avg Train loss 0.277063707765, Train acc 0.90585,Test acc 0.8899\n",
      "Epoch 69, Avg Train loss 0.27625604372, Train acc 0.906066666667,Test acc 0.8865\n",
      "Epoch 70, Avg Train loss 0.277107708486, Train acc 0.904,Test acc 0.8856\n",
      "Epoch 71, Avg Train loss 0.279283593591, Train acc 0.903416666667,Test acc 0.8836\n",
      "Epoch 72, Avg Train loss 0.276594399643, Train acc 0.912733333333,Test acc 0.8903\n",
      "Epoch 73, Avg Train loss 0.276519266733, Train acc 0.904666666667,Test acc 0.8888\n",
      "Epoch 74, Avg Train loss 0.276231119665, Train acc 0.910316666667,Test acc 0.8881\n",
      "Epoch 75, Avg Train loss 0.275605513668, Train acc 0.898016666667,Test acc 0.8826\n",
      "Epoch 76, Avg Train loss 0.279119057782, Train acc 0.907366666667,Test acc 0.8878\n",
      "Epoch 77, Avg Train loss 0.273961647097, Train acc 0.907816666667,Test acc 0.889\n",
      "Epoch 78, Avg Train loss 0.274496123695, Train acc 0.906333333333,Test acc 0.8887\n",
      "Epoch 79, Avg Train loss 0.274754643806, Train acc 0.9063,Test acc 0.8875\n",
      "Epoch 80, Avg Train loss 0.275496237055, Train acc 0.905483333333,Test acc 0.8853\n",
      "Epoch 81, Avg Train loss 0.27567494278, Train acc 0.904533333333,Test acc 0.8884\n",
      "Epoch 82, Avg Train loss 0.276221872489, Train acc 0.900783333333,Test acc 0.8826\n",
      "Epoch 83, Avg Train loss 0.276895400365, Train acc 0.9049,Test acc 0.8837\n",
      "Epoch 84, Avg Train loss 0.27355144186, Train acc 0.906616666667,Test acc 0.8869\n",
      "Epoch 85, Avg Train loss 0.274373145453, Train acc 0.904133333333,Test acc 0.8848\n",
      "Epoch 87, Avg Train loss 0.274174688435, Train acc 0.902266666667,Test acc 0.8821\n",
      "Epoch 88, Avg Train loss 0.27341101319, Train acc 0.905266666667,Test acc 0.8859\n",
      "Epoch 89, Avg Train loss 0.273441584365, Train acc 0.90575,Test acc 0.8827\n",
      "Epoch 90, Avg Train loss 0.275047842789, Train acc 0.910333333333,Test acc 0.8909\n",
      "Epoch 91, Avg Train loss 0.27303066295, Train acc 0.9074,Test acc 0.8875\n",
      "Epoch 92, Avg Train loss 0.2759245368, Train acc 0.902266666667,Test acc 0.8836\n",
      "Epoch 93, Avg Train loss 0.273305976963, Train acc 0.907116666667,Test acc 0.8905\n",
      "Epoch 94, Avg Train loss 0.272265178331, Train acc 0.90605,Test acc 0.8889\n",
      "Epoch 95, Avg Train loss 0.276146463903, Train acc 0.907916666667,Test acc 0.8865\n",
      "Epoch 96, Avg Train loss 0.273050324631, Train acc 0.90545,Test acc 0.8873\n",
      "Epoch 97, Avg Train loss 0.273244071579, Train acc 0.907933333333,Test acc 0.8864\n",
      "Epoch 98, Avg Train loss 0.273343979915, Train acc 0.9041,Test acc 0.8885\n",
      "Epoch 99, Avg Train loss 0.274343678315, Train acc 0.908516666667,Test acc 0.8891\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = .01\n",
    "batch_size = 128\n",
    "num_examples = 60000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = .0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    train_acc = utils.evaluate_accuracy_gluon(train_data, net)\n",
    "    test_acc = utils.evaluate_accuracy_gluon(test_data, net)\n",
    "    print(\"Epoch %s, Avg Train loss %s, Train acc %s,Test acc %s\" \n",
    "          % (epoch, cumulative_loss / num_examples, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
