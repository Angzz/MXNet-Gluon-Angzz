{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从0开始批量归一化\n",
    "\n",
    "对于层数比较深的神经网络，我们通常使用批量归一化来增加数据稳定性。举个例子，随着第一层和第二层的参数在训练时不断变化，第三层所使用的激活函数的输入值可能由于乘法效应而变得极大或极小，例如和第一层所使用的激活函数的输入值不在一个数量级上。**这种在训练时可能出现的情况会造成模型训练的不稳定性。**例如，给定一个学习率，某次参数迭代后，目标函数值会剧烈变化或甚至升高。数学解释是，我们如果把目标函数$f$根据参数$\\mathbf{w}$迭代$f(\\mathbf{w} - \\eta \\nabla f(\\mathbf{w}))$进行泰勒展开，有关学习率$\\eta$的高阶项的系数可能由于数量级的原因（通常由于层数多）而不容忽略。然而常用的低阶优化算法（如梯度下降）对于不断降低目标函数的有效性通常基于一个基本假设：在以上泰勒展开中把有关学习率的高阶项通通忽略不计，这就直接导致了不同层间目标函数值的输出不在同一个量级上。\n",
    "\n",
    "为了应对上述这种情况，Sergey Ioffe和Christian Szegedy在2015年提出了批量归一化的方法。简而言之，在训练时给定一个批量输入，批量归一化试图对深度学习模型的某一层所使用的激活函数的输入进行归一化：**<font color=\"red\">使批量呈标准正态分布（均值为0，标准差为1）。**\n",
    "    \n",
    "批量归一化通常应用于输入层或任意中间层。\n",
    "\n",
    "## 批量归一化层\n",
    "\n",
    "根据原作者论文中的阐述，批量归一化层不同于丢弃层，通常在激活层之前使用。\n",
    "\n",
    "其基本思想是对每一个批量($mini-batch$)的输入通过施加尺度变换$(scale\\,and\\,shift)$来进行归一化($normalization$)。\n",
    "\n",
    "具体来说，对每一个输入批量$B = \\{x_{1, ..., m}\\}$，我们需要学习拉伸参数$\\gamma$以及偏移参数$\\beta$，输出层变为$\\{y_i = BN_{\\gamma, \\beta}(x_i)\\}$，其中：\n",
    "\n",
    "$$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m}x_i$$\n",
    "\n",
    "$$\\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(x_i - \\mu_B)^2$$\n",
    "\n",
    "$$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "\n",
    "$$y_i \\leftarrow \\gamma \\hat{x_i} + \\beta \\equiv \\mbox{BN}_{\\gamma,\\beta}(x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，对于全连接层，很明显我们需要对每个批量进行归一化。而对于2维卷积，我们需要对每个通道中的``batch_size * height * width``做归一化，因此$\\gamma$和$\\beta$的长度与通道数一致。在我们的实现中，我们需要``reshape``我们的$\\gamma$和$\\beta$，使得它们可以与矩阵正确的相乘，并且我们要保持输入的四维形状使得其可以正确地广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:17.163589Z",
     "start_time": "2018-01-22T02:09:15.022797Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "mx.random.seed(1)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:29.967171Z",
     "start_time": "2018-01-22T02:10:17.164666Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data, test_data = utils.load_dataset(batch_size, data_type='fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:29.989334Z",
     "start_time": "2018-01-22T02:10:29.969738Z"
    }
   },
   "outputs": [],
   "source": [
    "def pure_batch_norm(X, gamma, beta, eps=1e-5):\n",
    "    if len(X.shape) not in [2,4]:\n",
    "        raise ValueError('the shape of the inputs must be 2 or 4.')\n",
    "    \n",
    "    if len(X.shape) == 2:\n",
    "        mean = nd.mean(X, axis=0, keepdims=True)\n",
    "        variance = nd.mean((X - mean)**2, axis=0, keepdims=True)\n",
    "    else:\n",
    "        mean = nd.mean(X, axis=(0,2,3), keepdims=True)\n",
    "        variance = nd.mean((X - mean)**2, axis=(0,2,3), keepdims=True)\n",
    "    \n",
    "    X_hat = 1.0*(X - mean) / nd.sqrt(variance + eps)\n",
    "    out = gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二维测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:38.478185Z",
     "start_time": "2018-01-22T02:10:29.991704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  2.]\n",
       " [ 3.  4.]]\n",
       "<NDArray 2x2 @gpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(4, ctx=ctx) + 1\n",
    "X = X.reshape((2,2))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:38.506637Z",
     "start_time": "2018-01-22T02:10:38.479248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.99999499 -0.99999499]\n",
       " [ 0.99999499  0.99999499]]\n",
       "<NDArray 2x2 @gpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga = nd.array([1,1], ctx=ctx)\n",
    "be = nd.array([0,0], ctx=ctx)\n",
    "pure_batch_norm(X, ga, be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四维测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:39.872870Z",
     "start_time": "2018-01-22T02:10:38.507769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 1.  6.]\n",
       "   [ 5.  7.]]\n",
       "\n",
       "  [[ 4.  3.]\n",
       "   [ 2.  5.]]]\n",
       "\n",
       "\n",
       " [[[ 6.  3.]\n",
       "   [ 2.  4.]]\n",
       "\n",
       "  [[ 5.  3.]\n",
       "   [ 2.  5.]]]]\n",
       "<NDArray 2x2x2x2 @gpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = nd.array([1,6,5,7,4,3,2,5,6,3,2,4,5,3,2,5,6], ctx=ctx).reshape((2,2,2,2))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:40.632611Z",
     "start_time": "2018-01-22T02:10:39.873950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[-1.63784397  0.88191599]\n",
       "   [ 0.37796399  1.38586795]]\n",
       "\n",
       "  [[ 0.30779248 -0.51298743]\n",
       "   [-1.33376741  1.12857234]]]\n",
       "\n",
       "\n",
       " [[[ 0.88191599 -0.62993997]\n",
       "   [-1.13389194 -0.12598799]]\n",
       "\n",
       "  [[ 1.12857234 -0.51298743]\n",
       "   [-1.33376741  1.12857234]]]]\n",
       "<NDArray 2x2x2x2 @gpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(B, ga, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:41.630954Z",
     "start_time": "2018-01-22T02:10:40.634033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[  0.   1.   2.]\n",
       "   [  3.   4.   5.]\n",
       "   [  6.   7.   8.]]\n",
       "\n",
       "  [[  9.  10.  11.]\n",
       "   [ 12.  13.  14.]\n",
       "   [ 15.  16.  17.]]]]\n",
       "<NDArray 1x2x3x3 @gpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B2 = nd.arange(18, ctx=ctx).reshape((1,2,3,3))\n",
    "B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:42.629244Z",
     "start_time": "2018-01-22T02:10:41.632004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[-1.54919219 -1.1618942  -0.7745961 ]\n",
       "   [-0.38729805  0.          0.38729805]\n",
       "   [ 0.7745961   1.1618942   1.54919219]]\n",
       "\n",
       "  [[-1.54919219 -1.1618942  -0.7745961 ]\n",
       "   [-0.38729805  0.          0.38729805]\n",
       "   [ 0.7745961   1.1618942   1.54919219]]]]\n",
       "<NDArray 1x2x3x3 @gpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(B2, ga, be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试时的批量归一化\n",
    "\n",
    "既然我们在训练时使用了批量归一化，那么一个很自然的问题是，我们在测试的时候需要使用批量归一化吗？\n",
    "\n",
    "答案是肯定的，不用的话，训练出的模型在测试集上就不可能准确，但是在测试时，我们需要对批量归一化做些改动，一般在测试时，**我们使用的是整个训练集的均值和方差，而不再是每个批量的均值和方差**，但我们不会傻傻的计算整个训练集的均值和方差，因为这样会带来极大的计算开销(试想成百上千万的训练集)，因此我们使用移动平均的思想来计算，即在训练阶段我们除了计算每个批量的均值和方差意外，我们使用移动平均的思想来计算总的``moving_mean``和``moving_variance``，那么在测试阶段我们直接使用这两个结果来近似整个训练集的均值和方差。\n",
    "\n",
    "下面我们实现批量归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:43.689993Z",
     "start_time": "2018-01-22T02:10:42.630378Z"
    }
   },
   "outputs": [],
   "source": [
    "# def batch_norm(X, gamma, beta, moving_momentum=0.9, eps=1e-5, scope_name='', is_training=True):\n",
    "#     # 定义全局变量；\n",
    "#     global _BN_MOVING_MEANS, _BN_MOVING_VARS\n",
    "    \n",
    "#     # X的维度必须是2或4\n",
    "#     assert len(X.shape) in [2,4]\n",
    "    \n",
    "#     if len(X.shape) == 2:\n",
    "#         mean = nd.mean(X, axis=0)\n",
    "#         variance = nd.mean((X - mean)**2, axis=0)\n",
    "#     else:\n",
    "#         mean = nd.mean(X, axis=(0,2,3), keepdims=True)\n",
    "#         variance = nd.mean((X - mean)**2, axis=(0,2,3), keepdims=True)\n",
    "         \n",
    "#     #################### 全局变量 #########################    \n",
    "#     try: # to access them\n",
    "#         _BN_MOVING_MEANS, _BN_MOVING_VARS\n",
    "#     except: # error, create them\n",
    "#         _BN_MOVING_MEANS, _BN_MOVING_VARS = {}, {}    \n",
    "        \n",
    "#     if scope_name not in _BN_MOVING_MEANS:\n",
    "#         _BN_MOVING_MEANS[scope_name] = mean\n",
    "#     else:\n",
    "#         _BN_MOVING_MEANS[scope_name] = _BN_MOVING_MEANS[scope_name] * moving_momentum + (1-moving_momentum) * mean\n",
    "    \n",
    "#     if scope_name not in _BN_MOVING_VARS:\n",
    "#         _BN_MOVING_VARS[scope_name] = variance  \n",
    "#     else:\n",
    "#         _BN_MOVING_VARS[scope_name] = _BN_MOVING_VARS[scope_name] * moving_momentum + (1-moving_momentum) * variance\n",
    "#     #################### 全局变量 #########################        \n",
    "        \n",
    "#     if is_training:\n",
    "#         X_hat = (X - mean) / nd.sqrt(variance + eps)\n",
    "#     else:\n",
    "#         X_hat = (X - _BN_MOVING_MEANS[scope_name]) / nd.sqrt(_BN_MOVING_VARS[scope_name] + eps)\n",
    "        \n",
    "#     return gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:44.324226Z",
     "start_time": "2018-01-22T02:10:43.692638Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, is_training,  moving_mean, moving_variance,\n",
    "               moving_momentum=0.9, eps=1e-5):\n",
    "    if len(X.shape) not in [2,4]:\n",
    "        raise ValueError(\"The dimension of the input must bt 2-d(Dense) or 4-d(Conv2d)\")\n",
    "        \n",
    "    if len(X.shape) == 2:\n",
    "        mean = nd.mean(X, axis=0, keepdims=True)\n",
    "        variance = nd.mean((X - mean)**2, axis=0, keepdims=True)\n",
    "    else:\n",
    "        mean = nd.mean(X, axis=(0,2,3), keepdims=True)\n",
    "        variance = nd.mean((X - mean)**2, axis=(0,2,3), keepdims=True)\n",
    "    \n",
    "    # 变形使得其可以正确广播             \n",
    "    moving_mean = moving_mean.reshape(mean.shape)\n",
    "    moving_variance = moving_variance.reshape(variance.shape)\n",
    "    \n",
    "    if is_training:\n",
    "        X_hat = (X - mean) / nd.sqrt(variance + eps)\n",
    "        # in-place operation\n",
    "        moving_mean[:] = moving_momentum * moving_mean + (1-moving_momentum) * mean\n",
    "        moving_variance[:] = moving_momentum * moving_variance + (1-moving_momentum) * variance\n",
    "    else:\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_variance + eps) \n",
    "        \n",
    "    out = gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义参数\n",
    "\n",
    "* 权重缩减 : 0.01\n",
    "* Conv1 : 20 output_channel, 3x3 kernel\n",
    "* Conv2 : 50 output_channel, 5X5 kernel\n",
    "* FC1 : 128 output\n",
    "* FC2 : 10 output\n",
    "\n",
    "### ``nd.random.normal``\n",
    "* loc : 生成数据的概率分布的均值，对应着整个分布的中心。\n",
    "* scale : 生成数据的概率分布的标准差，对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:45.147849Z",
     "start_time": "2018-01-22T02:10:44.325563Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_scale =.01\n",
    "num_output_conv1 = 20\n",
    "num_output_conv2 = 50\n",
    "num_output_fc1 = 128\n",
    "num_output_fc2 = 10\n",
    "\n",
    "############# first conv layer ############# \n",
    "W1 = nd.random.normal(shape=(num_output_conv1, 1, 3, 3), scale=weight_scale, ctx=ctx)\n",
    "b1 = nd.random.normal(shape=num_output_conv1, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "gamma1 = nd.random.normal(shape=num_output_conv1, scale=weight_scale, ctx=ctx)\n",
    "beta1 = nd.random.normal(shape=num_output_conv1, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "moving_mean1 = nd.zeros(num_output_conv1, ctx=ctx)\n",
    "moving_variance1 = nd.zeros(num_output_conv1, ctx=ctx)\n",
    "\n",
    "############# second conv layer ############# \n",
    "W2 = nd.random.normal(shape=(num_output_conv2, num_output_conv1, 5, 5), scale=weight_scale, ctx=ctx)\n",
    "b2 = nd.random.normal(shape=num_output_conv2, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "gamma2 = nd.random.normal(shape=num_output_conv2, scale=weight_scale, ctx=ctx)\n",
    "beta2 = nd.random.normal(shape=num_output_conv2, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "moving_mean2 = nd.zeros(num_output_conv2, ctx=ctx)  \n",
    "moving_variance2 = nd.zeros(num_output_conv2, ctx=ctx)\n",
    "\n",
    "############# first fc layer ############# \n",
    "W3 = nd.random.normal(shape=(800, num_output_fc1), scale=weight_scale, ctx=ctx)\n",
    "b3 = nd.random.normal(shape=num_output_fc1, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "gamma3 = nd.random.normal(shape=num_output_fc1, scale=weight_scale, ctx=ctx)\n",
    "beta3 = nd.random.normal(shape=num_output_fc1, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "# moving_mean3 = nd.zeros(num_output_fc1, ctx=ctx)\n",
    "# moving_variance3 = nd.zeros(num_output_fc1, ctx=ctx)\n",
    "\n",
    "############# second fc layer ############# \n",
    "\n",
    "W4 = nd.random.normal(shape=(num_output_fc1, num_output_fc2), scale=weight_scale, ctx=ctx)\n",
    "b4 = nd.random.normal(shape=num_output_fc2, scale=weight_scale, ctx=ctx)\n",
    "\n",
    "# moving_loss 和\\ moving_variance不需要更新\n",
    "params = [W1, b1, gamma1, beta1, W2, b2, gamma2, beta2, W3, b3, W4, b4]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:46.517506Z",
     "start_time": "2018-01-22T02:10:45.150056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# shape inference\n",
    "X = nd.random.normal(shape=(32,1,28,28), ctx=ctx)\n",
    "conv1 = nd.Convolution(data=X, weight=W1, bias=b1, kernel=W1.shape[2:], stride=(1,1), \n",
    "                       num_filter=W1.shape[0])\n",
    "pool1 = nd.Pooling(data=conv1, kernel=(2,2), pool_type='max', stride=(2,2))\n",
    "conv2 = nd.Convolution(data=pool1, weight=W2, bias=b2, kernel=W2.shape[2:], stride=(1,1), \n",
    "                       num_filter=W2.shape[0])\n",
    "pool2 = nd.Pooling(data=conv2, kernel=(2,2), pool_type='max', stride=(2,2))\n",
    "print(pool2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "conv -> norm -> relu -> pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:46.776342Z",
     "start_time": "2018-01-22T02:10:46.520556Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:48.760271Z",
     "start_time": "2018-01-22T02:10:46.777578Z"
    }
   },
   "outputs": [],
   "source": [
    "def net(X, is_training=False, verbose=False):\n",
    "    ############# first conv layer ############# \n",
    "    conv1 = nd.Convolution(data=X, weight=W1, bias=b1, kernel=W1.shape[2:], stride=(1,1), \n",
    "                       num_filter=W1.shape[0])\n",
    "    norm1 = batch_norm(conv1, gamma1, beta1, is_training, moving_mean1, moving_variance1)\n",
    "    relu1 = relu(norm1)\n",
    "    pool1 = nd.Pooling(data=relu1, pool_type='avg', kernel=(2,2), stride=(2,2))\n",
    "    \n",
    "    \n",
    "    ############# second conv layer ############# \n",
    "    conv2 = nd.Convolution(data=pool1, weight=W2, bias=b2, kernel=W2.shape[2:], stride=(1,1), \n",
    "                       num_filter=W2.shape[0])\n",
    "    norm2 = batch_norm(conv2, gamma2, beta2, is_training, moving_mean2, moving_variance2)\n",
    "    relu2 = relu(norm2)\n",
    "    pool2 = nd.Pooling(data=relu2, pool_type='avg', kernel=(2,2), stride=(2,2))\n",
    "\n",
    "    \n",
    "    ############# flatten layer #############\n",
    "    flatten = nd.Flatten(pool2)\n",
    "    \n",
    "    ############# first fc layer #############\n",
    "    fc1 = nd.dot(flatten, W3) + b3\n",
    "    # norm3 = batch_norm(fc1, gamma3, beta3, is_training, moving_mean3, moving_variance3)\n",
    "    relu3 = relu(fc1)\n",
    "    \n",
    "    ############# second fc layer #############\n",
    "    fc2 = nd.dot(relu3, W4) + b4\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"input shape : \", X.shape)\n",
    "        print(\"conv1 shape : \", conv1.shape)\n",
    "        print(\"pool1 shape : \", pool1.shape)\n",
    "        print(\"conv2 shape : \", conv2.shape)\n",
    "        print(\"pool2 shape : \", pool2.shape)\n",
    "        print(\"fc1 shape : \", fc1.shape)\n",
    "        print(\"output shape : \", fc2.shape)\n",
    "        \n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T02:10:50.385204Z",
     "start_time": "2018-01-22T02:10:48.761552Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-22T02:09:18.160Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(ylinear):\n",
    "    yexp = nd.exp(ylinear - nd.max(ylinear))\n",
    "    partition = nd.nansum(yexp, axis=1, keepdims=True) # keepdims相当于reshape((-1, 1))\n",
    "    return yexp / partition  \n",
    "\n",
    "def softmax_cross_entropy(yhat, y): \n",
    "    return - nd.nansum(y * nd.log(softmax(yhat)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-22T02:09:18.165Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "niter = 0\n",
    "moving_loss = .0\n",
    "smoothing_constant = 0.9   \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time()\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            # 训练阶段\n",
    "            output = net(data, is_training=True)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        \n",
    "        niter += 1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = smoothing_constant * moving_loss + (1 - smoothing_constant) * curr_loss\n",
    "        estimated_loss = moving_loss / (1 - smoothing_constant**niter)\n",
    "      \n",
    "    train_acc = utils.evaluate_accuracy_scratch(train_data, net, ctx)\n",
    "    test_acc = utils.evaluate_accuracy_scratch(test_data, net, ctx)\n",
    "    print(\"Epoch %d, Moving Train Avg loss %.5f, Train acc %.5f, Test acc %.5f, Time consume %.5f s.\"\n",
    "         % (epoch, estimated_loss, train_acc, test_acc, time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018年3月23日更新\n",
    "\n",
    "Kaiming He等人提出组归一化Group Norm\n",
    "\n",
    "论文地址：https://arxiv.org/abs/1803.08494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T07:12:45.366018Z",
     "start_time": "2018-03-23T07:12:45.349051Z"
    }
   },
   "outputs": [],
   "source": [
    "def group_norm(X, gamma, beta, G, eps=2e-5):\n",
    "    assert len(X.shape) == 4\n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    X = X.reshape((N, G, C // G, H, W))\n",
    "    mean = nd.mean(X, axis=(2, 3, 4), keepdims=True)\n",
    "    variance = nd.mean((X - mean)**2, axis=(2, 3, 4), keepdims=True)\n",
    "    \n",
    "    X_hat = 1.0*(X - mean) / nd.sqrt(variance + eps)\n",
    "    out = gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)\n",
    "    out = out.reshape((N, C, H, W))\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
