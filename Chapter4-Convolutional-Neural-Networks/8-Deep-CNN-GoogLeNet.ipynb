{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet - 更深的卷积神经网络\n",
    "\n",
    "<img src=\"http://zh.gluon.ai/_images/googlenet.png\" width=\"800\">\n",
    "\n",
    "GoogLeNet中有多个四个并行卷积层的块。这个块一般叫做Inception，其基于**Network in network**的思想做了很大的改进。我们先看下如何定义一个下图所示的Inception块。\n",
    "\n",
    "<img src=\"http://zh.gluon.ai/_images/inception.svg\" width=\"400\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T13:14:09.274928Z",
     "start_time": "2018-01-22T13:14:08.757180Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "\n",
    "ctx = mx.gpu()\n",
    "mx.random.seed(1)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T13:14:09.307097Z",
     "start_time": "2018-01-22T13:14:09.276271Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4条path\n",
    "class Inception(gluon.Block):\n",
    "    def __init__(self, n1_1, n2_1, n2_3, n3_1, n3_5, n4_1, debug=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.debug = debug\n",
    "        self.p1_conv_1 = nn.Conv2D(n1_1, kernel_size=1, activation='relu')\n",
    "        self.p2_conv_1 = nn.Conv2D(n2_1, kernel_size=1, activation='relu')\n",
    "        self.p2_conv_3 = nn.Conv2D(n2_3, kernel_size=3, padding=1, activation='relu')\n",
    "        self.p3_conv_1 = nn.Conv2D(n3_1, kernel_size=1, activation='relu')\n",
    "        self.p3_conv_5 = nn.Conv2D(n3_5, kernel_size=5, padding=2, activation='relu')\n",
    "        self.p4_pool_3 = nn.MaxPool2D(pool_size=3, padding=1, strides=1)\n",
    "        self.p4_conv_1 = nn.Conv2D(n4_1, kernel_size=1, activation='relu')\n",
    "        \n",
    "    def forward(self, X):\n",
    "        p1 = self.p1_conv_1(X)\n",
    "        p2 = self.p2_conv_3(self.p2_conv_1(X))\n",
    "        p3 = self.p3_conv_5(self.p3_conv_1(X))\n",
    "        p4 = self.p4_conv_1(self.p4_pool_3(X))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"p1 out shape : \", p1.shape)\n",
    "            print(\"p2 out shape : \", p2.shape)\n",
    "            print(\"p3 out shape : \", p3.shape)\n",
    "            print(\"p4 out shape : \", p4.shape)\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T13:14:10.752464Z",
     "start_time": "2018-01-22T13:14:09.308221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 out shape :  (32, 64, 64, 64)\n",
      "p2 out shape :  (32, 128, 64, 64)\n",
      "p3 out shape :  (32, 32, 64, 64)\n",
      "p4 out shape :  (32, 32, 64, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 256, 64, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incp = Inception(64, 96, 128, 16, 32, 32, debug=True)\n",
    "incp.initialize(ctx=ctx)\n",
    "\n",
    "X = nd.random.normal(shape=(32, 3, 64, 64), ctx=ctx)\n",
    "incp(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception的思想\n",
    "\n",
    "* 1.单个$1 \\times 1$卷积。\n",
    "* 2.$1 \\times 1$卷积接上$3 \\times 3$卷积。通常前者的通道数少于输入通道，这样减少后者的计算量。后者加上了$padding=1$使得输出的长宽和输入一致。\n",
    "* 3.同2，但换成了$5 \\times 5$卷积，因此$padding=2$才能保证输出的长宽和输入一致。\n",
    "* 4.和1类似，但卷积前用了最大池化层。\n",
    "\n",
    "## Inception提出的动机\n",
    "\n",
    "参考博文：[CNN架构优化之二：GoogLeNet Incepetion V1](https://zhuanlan.zhihu.com/p/31809031)\n",
    "\n",
    "一般而言，提升网络性能最直接的方法就是增加网络的深度和宽度，这也就意味着巨大的参数，这就更容易导致Over-fitting和巨大的计算量。解决上述两个缺点的根本方法是将全连接层甚至一般的卷积层都转化为稀疏连接。主要有两个原因：\n",
    "* 现实世界中的生物神经系统的连接也是稀疏的；\n",
    "* 文献《Provable bounds for learning some deep representations》表明，对于大规模稀疏的神经网络，可以通过分析激活值的统计特性和对高度相关的数据进行聚类来逐层构建出一个最优网络。这点表明臃肿的稀疏网络可能可以在不损失性能的前提下被简化;\n",
    "\n",
    "早些时候，为了打破网络对称性和提高学习能力，传统的网络都使用了随机稀疏连接，但是计算机软硬件对非对称稀疏数据的计算效率很差，所以在AlexNet中又重新启用了全连接层，目的是为了更好的优化并行运算。\n",
    "\n",
    "所以，现在的问题是有没有一种方法，既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能?大量文献表明,可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，据此，该论文提出了名为<font color=\"red\">**Inception**</font>的结构来实现这个方法。\n",
    "\n",
    "Inception结构的主要思路是如何用密集成分来近似最优的局部稀疏结构。因此作者首先提出了下图左的结构，之后为了减少参数，加入了$1 \\times 1$的卷积来进行降维，而此思路来自于Network in network。\n",
    "\n",
    "<img src=\"https://pic1.zhimg.com/80/v2-ca313c6a87d0ba064372171cc7d1be72_hd.jpg\">\n",
    "\n",
    "对上图的说明如下：\n",
    "\n",
    "* 采用不同大小的卷积核意味着不同大小视野或者感受野，最后的拼接意味着**不同尺度特征的融合**；\n",
    "\n",
    "* 之所以采用1、3、5大小的卷积核，主要是为了方便对齐，设定卷积步长stride=1之后，只要分别设定pad=0、1、2，则卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；\n",
    "\n",
    "* 很多文献表明Pooling效果很好，所以Inception也嵌入了Pooling；\n",
    "\n",
    "* 网络越深，特征越抽象，而且每个特征所涉及的感受野更大，因此随着层数的增加，3x3和5x5的卷积核比例也要增加。\n",
    "\n",
    "* 由于使用5x5的卷积核仍然会带来巨大的计算量，所以，文章借鉴了《Network in Network》中的方法，采用1x1卷积核来进行降维。例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32+32x5x5x256，大约减少了4倍。\n",
    "\n",
    "## GoogLeNet\n",
    "\n",
    "下图为GoogLeNet的详细的网络参数：\n",
    "\n",
    "<img src=\"../img/Chapter4-Convolutional-Neural-Networks/4-2.png\" width=\"850\">\n",
    "\n",
    "对该网络的说明如下：\n",
    "\n",
    "* GoogLeNet采用了模块化的结构，方便添加和修改；\n",
    "\n",
    "* 网络最后采用了average pooling来代替全连接层，想法来自《Network in Network》，事实证明可以将TOP1 accuracy提高0.6%；\n",
    "\n",
    "* 虽然移除了全连接层，但是网络中依旧使用可Dropout；\n",
    "\n",
    "* 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。文章中说这两个辅助的分类器的loss应该加一个衰减系数，但看caffe中的model也没有加任何衰减。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们只实现GoogLeNet中的一个输出，并将整个网络分为六个阶段，每个阶段定义一个``gluon.nn.Squential``，最后再把所有的这些块连接起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T13:14:10.823670Z",
     "start_time": "2018-01-22T13:14:10.753692Z"
    }
   },
   "outputs": [],
   "source": [
    "class GoogLeNet(gluon.Block):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.verbose = verbose\n",
    "            # first block(same padding)\n",
    "            b1 = gluon.nn.Sequential()\n",
    "            b1.add(\n",
    "                gluon.nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "                gluon.nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "            # second block\n",
    "            b2 = gluon.nn.Sequential()\n",
    "            b2.add(\n",
    "                gluon.nn.Conv2D(64, kernel_size=1), # 3X3 reduce\n",
    "                gluon.nn.Conv2D(192, kernel_size=3, strides=1, padding=1, activation='relu'), # 3X3\n",
    "                gluon.nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "            # Inception 3a (64, 96, 128, 16, 32, 32)\n",
    "            # Inception 3b (128, 128, 192, 32, 96, 64)\n",
    "            b3 = gluon.nn.Sequential()\n",
    "            b3.add(\n",
    "                Inception(64, 96, 128, 16, 32, 32),\n",
    "                Inception(128, 128, 192, 32, 96, 64),\n",
    "                gluon.nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "            # Inception (4a) (192, 96, 208, 16, 48, 64)\n",
    "            # Inception (4b) (160, 112, 224, 24, 64, 64)\n",
    "            # Inception (4c) (128, 128, 256, 24, 64, 64)\n",
    "            # Inception (4d) (112, 144, 288, 32, 64, 64)\n",
    "            # Inception (4e) (256, 160, 320, 32, 128, 128)\n",
    "            b4 = gluon.nn.Sequential()\n",
    "            b4.add(\n",
    "                Inception(192, 96, 208, 16, 48, 64),\n",
    "                Inception(160, 112, 224, 24, 64, 64),\n",
    "                Inception(128, 128, 256, 24, 64, 64),\n",
    "                Inception(112, 144, 288, 32, 64, 64),\n",
    "                Inception(256, 160, 320, 32, 128, 128),\n",
    "                gluon.nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "            # Inception (5a) (256, 160, 320, 32, 128, 128)\n",
    "            # Inception (5b) (384, 192, 384, 48, 128, 128)\n",
    "            b5 = gluon.nn.Sequential()\n",
    "            b5.add(\n",
    "                Inception(256, 160, 320, 32, 128, 128),\n",
    "                Inception(384, 192, 384, 48, 128, 128),\n",
    "                # 这里原论文是7，但是因为我们训练的输入尺寸只有96， 因为我们改为2\n",
    "                gluon.nn.AvgPool2D(pool_size=2, strides=1)\n",
    "            )\n",
    "            b6 = gluon.nn.Sequential()\n",
    "            b6.add(\n",
    "                gluon.nn.Dropout(0.4),\n",
    "                gluon.nn.Flatten(),\n",
    "                gluon.nn.Dense(num_classes),\n",
    "            )\n",
    "            # chain block together\n",
    "            self.net = gluon.nn.Sequential()\n",
    "            self.net.add(b1, b2, b3, b4, b5, b6)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for i, blk in enumerate(self.net):\n",
    "            out = blk(out)\n",
    "            if self.verbose:\n",
    "                print(\"blk%d output : %s\" % (i+1, out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T13:14:11.501564Z",
     "start_time": "2018-01-22T13:14:10.824821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blk1 output : (32, 64, 23, 23)\n",
      "blk2 output : (32, 192, 11, 11)\n",
      "blk3 output : (32, 480, 5, 5)\n",
      "blk4 output : (32, 832, 2, 2)\n",
      "blk5 output : (32, 1024, 1, 1)\n",
      "blk6 output : (32, 10)\n"
     ]
    }
   ],
   "source": [
    "gln = GoogLeNet(num_classes=10, verbose=True)\n",
    "gln.initialize(ctx=ctx)\n",
    "\n",
    "X = nd.random.normal(shape=(32, 3, 96, 96), ctx=ctx)\n",
    "y = gln(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T13:37:03.840104Z",
     "start_time": "2018-01-22T13:14:11.503036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Moving Train Avg loss 2.03005, Train acc 0.28000, Test acc 0.28670, Time consume 69.89280 s.\n",
      "Epoch 1, Moving Train Avg loss 1.69580, Train acc 0.41744, Test acc 0.41910, Time consume 68.78216 s.\n",
      "Epoch 2, Moving Train Avg loss 1.44750, Train acc 0.50410, Test acc 0.49940, Time consume 68.90851 s.\n",
      "Epoch 3, Moving Train Avg loss 1.15227, Train acc 0.59978, Test acc 0.57780, Time consume 68.84130 s.\n",
      "Epoch 4, Moving Train Avg loss 1.04971, Train acc 0.64476, Test acc 0.62740, Time consume 68.90965 s.\n",
      "Epoch 5, Moving Train Avg loss 0.92597, Train acc 0.69340, Test acc 0.66270, Time consume 68.88063 s.\n",
      "Epoch 6, Moving Train Avg loss 0.85547, Train acc 0.74402, Test acc 0.69830, Time consume 68.98737 s.\n",
      "Epoch 7, Moving Train Avg loss 0.83491, Train acc 0.78594, Test acc 0.73010, Time consume 68.87968 s.\n",
      "Epoch 8, Moving Train Avg loss 0.71385, Train acc 0.81424, Test acc 0.74510, Time consume 68.92734 s.\n",
      "Epoch 9, Moving Train Avg loss 0.61888, Train acc 0.80940, Test acc 0.73530, Time consume 68.90568 s.\n",
      "Epoch 10, Moving Train Avg loss 0.64113, Train acc 0.84474, Test acc 0.75360, Time consume 68.72857 s.\n",
      "Epoch 11, Moving Train Avg loss 0.56407, Train acc 0.85668, Test acc 0.75980, Time consume 68.13550 s.\n",
      "Epoch 12, Moving Train Avg loss 0.44632, Train acc 0.87670, Test acc 0.76650, Time consume 68.12445 s.\n",
      "Epoch 13, Moving Train Avg loss 0.49398, Train acc 0.88038, Test acc 0.76340, Time consume 68.07581 s.\n",
      "Epoch 14, Moving Train Avg loss 0.34250, Train acc 0.91022, Test acc 0.78060, Time consume 68.05626 s.\n",
      "Epoch 15, Moving Train Avg loss 0.51394, Train acc 0.90140, Test acc 0.77020, Time consume 68.07132 s.\n",
      "Epoch 16, Moving Train Avg loss 0.46752, Train acc 0.81872, Test acc 0.70870, Time consume 68.10977 s.\n",
      "Epoch 17, Moving Train Avg loss 0.31915, Train acc 0.89584, Test acc 0.75620, Time consume 68.08735 s.\n",
      "Epoch 18, Moving Train Avg loss 0.26410, Train acc 0.91662, Test acc 0.76760, Time consume 68.08381 s.\n",
      "Epoch 19, Moving Train Avg loss 0.32007, Train acc 0.93688, Test acc 0.78180, Time consume 68.08315 s.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "batch_size = 64\n",
    "train_data, test_data = utils.load_dataset(batch_size, resize=96, data_type='cifar10')\n",
    "\n",
    "gln = GoogLeNet(num_classes=10, verbose=False)\n",
    "gln.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(gln.collect_params(), 'sgd', {'learning_rate' : 0.1})\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "niter = 0\n",
    "moving_loss = 0.0\n",
    "smoothing_constant = 0.9\n",
    "\n",
    "from time import time\n",
    "for epoch in range(epochs):\n",
    "    start = time()\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = gln(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        niter += 1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = smoothing_constant * moving_loss + (1-smoothing_constant) * curr_loss\n",
    "        estimated_loss = moving_loss / (1 - smoothing_constant**niter)\n",
    "    \n",
    "    train_acc = utils.evaluate_accuracy_gluon(train_data, gln, ctx)\n",
    "    test_acc = utils.evaluate_accuracy_gluon(test_data, gln, ctx)\n",
    "    print(\"Epoch %d, Moving Train Avg loss %.5f, Train acc %.5f, Test acc %.5f, Time consume %.5f s.\"\n",
    "         % (epoch, estimated_loss, train_acc, test_acc, time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
