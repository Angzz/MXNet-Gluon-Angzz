{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深层卷积神经网络 - AlexNet\n",
    "\n",
    "1.制约深度学习发展的两个因素有\n",
    "* 数据\n",
    "* 硬件\n",
    "深度学习是指通过学习浅层特征层层递进，进而可以表示出高层语义，是一种层进式的结构。\n",
    "\n",
    "2.**AlexNet**\n",
    "\n",
    "* 与相对较小的LeNet相比，AlexNet包含8层变换，其中有五层卷积和两层全连接隐含层，以及一个输出层。\n",
    "\n",
    "* 第一层中卷积核的大小是$11 \\times 11$，接着第二层中的是$5 \\times 5$，之后都是$3 \\times 3$。此外，第一，第二和第五个卷积层之后都跟了有重叠的大小为$3 \\times 3$，步距为$2 \\times 2$的池化操作。\n",
    "\n",
    "* 五层卷积层的输出通道数分别为\\[96, 256, 384, 384, 256\\]\n",
    "\n",
    "* 紧接着卷积层，原版的AlexNet有每层大小为4096个节点的全连接层，这两个巨大的全连接层带来将近1GB的模型大小。\n",
    "\n",
    "下面我们实现AlexNet。\n",
    "\n",
    "我们将图片resize到$224 \\times 224$，这是原论文中的图片尺寸，我们使用CIFAR10数据集。\n",
    "\n",
    "另外科普一下$ImageNet$，包含1000个类别，100万张图片，每张图片大小为$256 \\times 256$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:10.381321Z",
     "start_time": "2018-01-20T05:52:09.888655Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "import utils\n",
    "\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:11.162876Z",
     "start_time": "2018-01-20T05:52:10.382568Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data, test_data = utils.load_dataset(batch_size, resize=224, data_type='cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:11.228230Z",
     "start_time": "2018-01-20T05:52:11.164106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "for data, _ in train_data:\n",
    "    data = data.as_in_context(ctx)\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet**\n",
    "\n",
    "* 与相对较小的LeNet相比，AlexNet包含8层变换，其中有五层卷积和两层全连接隐含层，以及一个输出层。\n",
    "\n",
    "* 第一层中卷积核的大小是$11 \\times 11$，接着第二层中的是$5 \\times 5$，之后都是$3 \\times 3$。此外，第一，第二和第五个卷积层之后都跟了有重叠的大小为$3 \\times 3$，步距为$2 \\times 2$的池化操作。\n",
    "\n",
    "* 五层卷积层的输出通道数分别为\\[96, 256, 384, 384, 256\\]\n",
    "\n",
    "* 紧接着卷积层，原版的AlexNet有每层大小为4096个节点的全连接层，这两个巨大的全连接层带来将近1GB的模型大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:11.259432Z",
     "start_time": "2018-01-20T05:52:11.230006Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        # 1st Conv Layer\n",
    "        net.add(gluon.nn.Conv2D(96, kernel_size=(11, 11), strides=(4,4), activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "        # 2nd Conv Layer\n",
    "        net.add(gluon.nn.Conv2D(256, kernel_size=(5,5), strides=(1,1), activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "        # 3rd Conv Layer \n",
    "        net.add(gluon.nn.Conv2D(384, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "        # 4th Conv Layer \n",
    "        net.add(gluon.nn.Conv2D(384, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "        # 5th Conv Layer \n",
    "        net.add(gluon.nn.Conv2D(256, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "        # 6th fc1 Layer \n",
    "        net.add(gluon.nn.Flatten()) # Flatten\n",
    "        net.add(gluon.nn.Dense(4096, activation='relu'))\n",
    "        # 7th fc2 Layer\n",
    "        net.add(gluon.nn.Dense(4096, activation='relu'))\n",
    "        # 8th output Layer \n",
    "        net.add(gluon.nn.Dense(10, activation='relu'))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:11.272135Z",
     "start_time": "2018-01-20T05:52:11.260985Z"
    }
   },
   "outputs": [],
   "source": [
    "net = get_net()\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx, force_reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:11.280744Z",
     "start_time": "2018-01-20T05:52:11.273318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter sequential0_conv0_weight (shape=(96, 0, 11, 11), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv0_bias (shape=(96,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv1_weight (shape=(256, 0, 5, 5), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv1_bias (shape=(256,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv2_weight (shape=(384, 0, 3, 3), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv2_bias (shape=(384,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv3_weight (shape=(384, 0, 3, 3), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv3_bias (shape=(384,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv4_weight (shape=(256, 0, 3, 3), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_conv4_bias (shape=(256,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_dense0_weight (shape=(4096, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_dense0_bias (shape=(4096,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_dense1_weight (shape=(4096, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_dense1_bias (shape=(4096,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_dense2_weight (shape=(10, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential0_dense2_bias (shape=(10,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:12.755696Z",
     "start_time": "2018-01-20T05:52:11.281937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:12.765742Z",
     "start_time": "2018-01-20T05:52:12.760248Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T05:52:12.776450Z",
     "start_time": "2018-01-20T05:52:12.770089Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:13:13.844926Z",
     "start_time": "2018-01-20T05:52:12.779108Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0, Train Moving Loss 2.29514025723, Train acc 0.12506, Test acc 0.1222, Time consume 126.42092108726501.\n",
      "Epoch 1, Train Moving Loss 2.19277653277, Train acc 0.22118, Test acc 0.2248, Time consume 126.10781288146973.\n",
      "Epoch 2, Train Moving Loss 2.0417418976, Train acc 0.29554, Test acc 0.2965, Time consume 126.2792866230011.\n",
      "Epoch 3, Train Moving Loss 1.89395673118, Train acc 0.36768, Test acc 0.367, Time consume 125.96690487861633.\n",
      "Epoch 4, Train Moving Loss 1.79321873896, Train acc 0.39568, Test acc 0.3849, Time consume 125.86386466026306.\n",
      "Epoch 5, Train Moving Loss 1.70088283089, Train acc 0.42492, Test acc 0.4016, Time consume 126.08402061462402.\n",
      "Epoch 6, Train Moving Loss 1.6306769425, Train acc 0.48108, Test acc 0.4435, Time consume 125.99323844909668.\n",
      "Epoch 7, Train Moving Loss 1.54959068667, Train acc 0.4847, Test acc 0.4361, Time consume 126.04888653755188.\n",
      "Epoch 8, Train Moving Loss 1.49294990862, Train acc 0.52216, Test acc 0.4523, Time consume 126.0649311542511.\n",
      "Epoch 9, Train Moving Loss 1.41745024167, Train acc 0.55576, Test acc 0.4672, Time consume 126.20735931396484.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "niter = 0\n",
    "moving_loss = .0\n",
    "smoothing_constant = 0.01\n",
    "\n",
    "print(\"Start training on \", ctx)\n",
    "for epoch in range(epochs):\n",
    "    start = time()\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        niter += 1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss\n",
    "        estimated_loss = moving_loss / (1 - (1-smoothing_constant)**niter)\n",
    "        \n",
    "    train_acc = utils.evaluate_accuracy_gluon(train_data, net, ctx)\n",
    "    test_acc = utils.evaluate_accuracy_gluon(test_data, net, ctx)\n",
    "    \n",
    "    print(\"Epoch %s, Train Moving Loss %s, Train acc %s, Test acc %s, Time consume %s s.\"\n",
    "         % (epoch, estimated_loss, train_acc, test_acc, time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T06:15:18.423085Z",
     "start_time": "2018-01-20T06:15:18.262133Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'models/alexnet-cifar10-0000.params'\n",
    "net.save_params(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
